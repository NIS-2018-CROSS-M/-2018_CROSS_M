{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "krc final.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6yPpvWqtFgqG"
      },
      "source": [
        "## Download varidal files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6HftlYTDlaSk",
        "colab": {}
      },
      "source": [
        "!wget https://raw.githubusercontent.com/ftyers/vardial-shared-task/master/test/trk-covered"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w9sPEAZQlcbl",
        "colab": {}
      },
      "source": [
        "!wget https://raw.githubusercontent.com/ftyers/vardial-shared-task/master/train/trk-uncovered"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9j2kAr0SihLv",
        "colab_type": "text"
      },
      "source": [
        "## Download transliterated files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3D5lZjramOyb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://raw.githubusercontent.com/NIS-2018-CROSS-M/vardial-shared-task/master/train/trk-uncovered-transliterated"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9pqDOvXjYTO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://raw.githubusercontent.com/NIS-2018-CROSS-M/vardial-shared-task/master/test/trk-covered-transliterated"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69IDI1lnnxoD",
        "colab_type": "code",
        "outputId": "93ee1cf2-b1ab-484c-9e7b-e4a23a36d128",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "source": [
        "!head trk-uncovered"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tat\tказак\tказак\tNOUN\tCase=Nom\n",
            "tur\tmuamele\tmuamele\tNOUN\tCase=Nom\n",
            "tat\tзаманы\tзаман\tNOUN\tCase=Nom|Number[psor]=Sing,Plur|Person[psor]=3\n",
            "tur\tkonut\tkonut\tNOUN\tCase=Nom\n",
            "tur\tkoruması\tkoru\tVERB\tCase=Nom|Number[psor]=Sing,Plur|Person[psor]=3|Valency=2|VerbForm=Vnoun\n",
            "tur\tkoruması\tkoruma\tNOUN\tCase=Nom|Number[psor]=Sing,Plur|Person[psor]=3\n",
            "kaz\tқоңыз\tқоңыз\tNOUN\tCase=Nom\n",
            "kaz\tорналасқандықтан\tорналас\tVERB\tAspect=Perf|Case=Abl|Valency=1|VerbForm=Vnoun\n",
            "kir\tжазуусун\tжаз\tVERB\tCase=Acc|Number[psor]=Sing,Plur|Person[psor]=3|Valency=2|VerbForm=Vnoun\n",
            "kir\tжазуусун\tжазуу\tNOUN\tCase=Acc|Number[psor]=Sing,Plur|Person[psor]=3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOBZqVm_n1sl",
        "colab_type": "code",
        "outputId": "4637adc4-94d1-4b9a-9493-e556be796c0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "source": [
        "!head trk-uncovered-transliterated"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tat\tказак\tказак\tNOUN\tCase=Nom\n",
            "tur\tмуамеле\tмуамеле\tNOUN\tCase=Nom\n",
            "tat\tзаманы\tзаман\tNOUN\tCase=Nom|Number[psor]=Sing,Plur|Person[psor]=3\n",
            "tur\tконут\tконут\tNOUN\tCase=Nom\n",
            "tur\tкорумасы\tкору\tVERB\tCase=Nom|Number[psor]=Sing,Plur|Person[psor]=3|Valency=2|VerbForm=Vnoun\n",
            "tur\tкорумасы\tкорума\tNOUN\tCase=Nom|Number[psor]=Sing,Plur|Person[psor]=3\n",
            "kaz\tконыз\tконыз\tNOUN\tCase=Nom\n",
            "kaz\tорналаскандыктан\tорналас\tVERB\tAspect=Perf|Case=Abl|Valency=1|VerbForm=Vnoun\n",
            "kir\tжазуусун\tжаз\tVERB\tCase=Acc|Number[psor]=Sing,Plur|Person[psor]=3|Valency=2|VerbForm=Vnoun\n",
            "kir\tжазуусун\tжазуу\tNOUN\tCase=Acc|Number[psor]=Sing,Plur|Person[psor]=3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aX1346vbjfO-",
        "colab_type": "code",
        "outputId": "8d6a90d7-cc14-49fe-82a1-9685089648da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "source": [
        "!head trk-covered"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "krc\tджыл\t_\t_\t_\n",
            "krc\tэм\t_\t_\t_\n",
            "krc\tджылны\t_\t_\t_\n",
            "krc\tболгъанды\t_\t_\t_\n",
            "krc\tдери\t_\t_\t_\n",
            "krc\tкёре\t_\t_\t_\n",
            "krc\tсора\t_\t_\t_\n",
            "krc\tуллу\t_\t_\t_\n",
            "krc\tболады\t_\t_\t_\n",
            "krc\tджылда\t_\t_\t_\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCbvJdQ0jmZb",
        "colab_type": "code",
        "outputId": "e743fa3d-ae0e-4357-fca7-70de502db316",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "source": [
        "!head trk-covered-transliterated"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "krc\tджыл\t_\t_\t_\n",
            "krc\tэм\t_\t_\t_\n",
            "krc\tджылны\t_\t_\t_\n",
            "krc\tболганды\t_\t_\t_\n",
            "krc\tдери\t_\t_\t_\n",
            "krc\tкёре\t_\t_\t_\n",
            "krc\tсора\t_\t_\t_\n",
            "krc\tуллу\t_\t_\t_\n",
            "krc\tболады\t_\t_\t_\n",
            "krc\tджылда\t_\t_\t_\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Cw4eOk9dDnzD"
      },
      "source": [
        "## Prepare train and test files for morfessor within 5 folds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmELGn8A1ELG",
        "colab_type": "code",
        "outputId": "8d0ed74e-a791-424e-ecd1-9d562c4f825e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "source": [
        "# wordforms extraction, rename the file for convenience\n",
        "!cut -f2 trk-covered-transliterated > trk-cov\n",
        "!head trk-cov"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "джыл\n",
            "эм\n",
            "джылны\n",
            "болганды\n",
            "дери\n",
            "кёре\n",
            "сора\n",
            "уллу\n",
            "болады\n",
            "джылда\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YsLQSMfZDu5p"
      },
      "source": [
        "### fold 1\n",
        "#### train = head 0.8n; test = tail 0.2n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_TpKKcN8BF3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "outputId": "0419346e-75a5-4b25-fa48-6d29e546801f"
      },
      "source": [
        "# train\n",
        "!head -$(cat trk-cov | python -c 'import sys; print(round(len([line.strip() for line \\\n",
        "in sys.stdin]) * 0.8))') trk-cov > train1.txt\n",
        "!wc -l train1.txt\n",
        "# test\n",
        "!tail -$(cat trk-cov | python -c 'import sys; print(round(len([line.strip() for line \\\n",
        "in sys.stdin]) * 0.2))') trk-cov > test1.txt\n",
        "!wc -l test1.txt\n",
        "# morfessor\n",
        "!morfessor -t train1.txt -T test1.txt > krc_segmented.txt\n",
        "!wc -l krc_segmented.txt"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7918 train1.txt\n",
            "1980 test1.txt\n",
            "Reading corpus from 'train1.txt'...\n",
            "Detected utf-8 encoding\n",
            "Done.\n",
            "Compounds in training data: 7025 types / 7025 tokens\n",
            "Starting batch training\n",
            "Epochs: 0\tCost: 209906.43988580024\n",
            "100% (7025 of 7025) |####################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
            "Epochs: 1\tCost: 143941.70254056508\n",
            "100% (7025 of 7025) |####################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
            "Epochs: 2\tCost: 136368.45241682202\n",
            "100% (7025 of 7025) |####################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
            "Epochs: 3\tCost: 135218.33679594586\n",
            "100% (7025 of 7025) |####################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
            "Epochs: 4\tCost: 134792.7560260337\n",
            "100% (7025 of 7025) |####################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
            "Epochs: 5\tCost: 134578.62951433798\n",
            "100% (7025 of 7025) |####################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
            "Epochs: 6\tCost: 134551.22576134145\n",
            "Done.\n",
            "Epochs: 6\n",
            "Final cost: 134551.22576134145\n",
            "Training time: 25.743s\n",
            "Segmenting test data...\n",
            "Reading corpus from 'test1.txt'...\n",
            "Done.\n",
            "\n",
            "Done.\n",
            "1980 krc_segmented.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1fD2Xv11ELV",
        "colab_type": "text"
      },
      "source": [
        "### fold 2\n",
        "#### train = head 0.75n train1 + test1; test = tail 0.25n train1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyPXAlc31ELX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "outputId": "8bb8af0a-3665-4ae1-e38f-badac692a119"
      },
      "source": [
        "# train\n",
        "!head -$(cat train1.txt | python -c 'import sys; print(round(len([line.strip() for line \\\n",
        "in sys.stdin]) * 0.75))') train1.txt >> train2.txt && cat test1.txt >> train2.txt\n",
        "!wc -l train2.txt\n",
        "# test\n",
        "!tail -$(cat train1.txt | python -c 'import sys; print(round(len([line.strip() for line \\\n",
        "in sys.stdin]) * 0.25))') train1.txt > test2.txt\n",
        "!wc -l test2.txt\n",
        "# morfessor\n",
        "!morfessor -t train1.txt -T test1.txt >> krc_segmented.txt\n",
        "!wc -l krc_segmented.txt"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7918 train2.txt\n",
            "1980 test2.txt\n",
            "Reading corpus from 'train1.txt'...\n",
            "Detected utf-8 encoding\n",
            "Done.\n",
            "Compounds in training data: 7025 types / 7025 tokens\n",
            "Starting batch training\n",
            "Epochs: 0\tCost: 209906.43988580024\n",
            "100% (7025 of 7025) |####################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
            "Epochs: 1\tCost: 144024.85050949\n",
            "100% (7025 of 7025) |####################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
            "Epochs: 2\tCost: 136644.51137275557\n",
            "100% (7025 of 7025) |####################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
            "Epochs: 3\tCost: 135405.7756779247\n",
            "100% (7025 of 7025) |####################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
            "Epochs: 4\tCost: 134977.50395370246\n",
            "100% (7025 of 7025) |####################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
            "Epochs: 5\tCost: 134849.16336096515\n",
            "100% (7025 of 7025) |####################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
            "Epochs: 6\tCost: 134810.18416130354\n",
            "100% (7025 of 7025) |####################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
            "Epochs: 7\tCost: 134794.85217235066\n",
            "Done.\n",
            "Epochs: 7\n",
            "Final cost: 134794.85217235066\n",
            "Training time: 30.207s\n",
            "Segmenting test data...\n",
            "Reading corpus from 'test1.txt'...\n",
            "Done.\n",
            "\n",
            "Done.\n",
            "3960 krc_segmented.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgV6DO3j1ELc",
        "colab_type": "text"
      },
      "source": [
        "### fold 3\n",
        "#### train = head 0.5n train1 + test1 + test2; test = tail 0.3n train1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOEAieEo1ELe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "outputId": "34b451dc-7cca-4402-b471-f23a76e67307"
      },
      "source": [
        "# train\n",
        "!head -$(cat train1.txt | python -c 'import sys; print(round(len([line.strip() for line \\\n",
        "in sys.stdin]) * 0.5 - 1))') train1.txt >> train3.txt && cat test1.txt >> train3.txt && cat test2.txt >> train3.txt\n",
        "!wc -l train3.txt\n",
        "# test\n",
        "!head -$(cat train1.txt | python -c 'import sys; print(round(len([line.strip() for line \\\n",
        "in sys.stdin]) * 0.75))') train1.txt | tail -$(wc -l test1.txt | cut -f1 -d' ') > test3.txt\n",
        "!wc -l test3.txt\n",
        "# morfessor\n",
        "!morfessor -t train3.txt -T test3.txt >> krc_segmented.txt\n",
        "!wc -l krc_segmented.txt"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7918 train3.txt\n",
            "1980 test3.txt\n",
            "Reading corpus from 'train3.txt'...\n",
            "Detected utf-8 encoding\n",
            "Done.\n",
            "Compounds in training data: 7136 types / 7136 tokens\n",
            "Starting batch training\n",
            "Epochs: 0\tCost: 217262.9882762189\n",
            "100% (7136 of 7136) |####################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
            "Epochs: 1\tCost: 151150.11020515187\n",
            "100% (7136 of 7136) |####################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
            "Epochs: 2\tCost: 142741.69848460887\n",
            "100% (7136 of 7136) |####################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
            "Epochs: 3\tCost: 141573.88567508644\n",
            "100% (7136 of 7136) |####################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
            "Epochs: 4\tCost: 141197.52387825274\n",
            "100% (7136 of 7136) |####################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
            "Epochs: 5\tCost: 141016.74215754768\n",
            "100% (7136 of 7136) |####################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
            "Epochs: 6\tCost: 140903.20354323083\n",
            "100% (7136 of 7136) |####################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
            "Epochs: 7\tCost: 140895.81478142526\n",
            "Done.\n",
            "Epochs: 7\n",
            "Final cost: 140895.81478142526\n",
            "Training time: 30.968s\n",
            "Segmenting test data...\n",
            "Reading corpus from 'test3.txt'...\n",
            "Done.\n",
            "\n",
            "Done.\n",
            "5940 krc_segmented.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hwt7R3w81ELk",
        "colab_type": "text"
      },
      "source": [
        "### fold 4\n",
        "#### train =  tail 0.8n; test = head 0.2n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHX1INyQ1ELm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "outputId": "14ec4b32-433f-42d6-f195-ac2fff474b26"
      },
      "source": [
        "# train\n",
        "!tail -$(cat trk-cov | python -c 'import sys; print(round(len([line.strip() for line \\\n",
        "in sys.stdin]) * 0.8))') trk-cov > train4.txt\n",
        "!wc -l train4.txt\n",
        "# test\n",
        "!head -$(cat trk-cov | python -c 'import sys; print(round(len([line.strip() for line \\\n",
        "in sys.stdin]) * 0.2))') trk-cov > test4.txt\n",
        "!wc -l test4.txt\n",
        "# morfessor\n",
        "!morfessor -t train4.txt -T test4.txt >> krc_segmented.txt\n",
        "!wc -l krc_segmented.txt"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7918 train4.txt\n",
            "1980 test4.txt\n",
            "Reading corpus from 'train4.txt'...\n",
            "Detected utf-8 encoding\n",
            "Done.\n",
            "Compounds in training data: 7499 types / 7499 tokens\n",
            "Starting batch training\n",
            "Epochs: 0\tCost: 232806.31241243417\n",
            "100% (7499 of 7499) |####################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
            "Epochs: 1\tCost: 167239.96746537054\n",
            "100% (7499 of 7499) |####################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
            "Epochs: 2\tCost: 154658.5896259652\n",
            "100% (7499 of 7499) |####################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
            "Epochs: 3\tCost: 152693.79129348206\n",
            "100% (7499 of 7499) |####################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
            "Epochs: 4\tCost: 152156.76427417988\n",
            "100% (7499 of 7499) |####################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
            "Epochs: 5\tCost: 151909.38308598223\n",
            "100% (7499 of 7499) |####################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
            "Epochs: 6\tCost: 151830.66168922724\n",
            "100% (7499 of 7499) |####################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
            "Epochs: 7\tCost: 151798.70587118898\n",
            "Done.\n",
            "Epochs: 7\n",
            "Final cost: 151798.70587118898\n",
            "Training time: 34.051s\n",
            "Segmenting test data...\n",
            "Reading corpus from 'test4.txt'...\n",
            "Done.\n",
            "\n",
            "Done.\n",
            "7920 krc_segmented.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBTT1n3Y1ELr",
        "colab_type": "text"
      },
      "source": [
        "### fold 5\n",
        "#### train = tail 0.75n train4 + test4; test = head train4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PD-g--fg1ELt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "outputId": "18fb5130-cd42-41f9-d091-3efbfdad0501"
      },
      "source": [
        "# train\n",
        "!tail -$(cat train4.txt | python -c 'import sys; print(round(len([line.strip() for line \\\n",
        "in sys.stdin]) * 0.75))') train4.txt >> train5.txt && cat test4.txt >> train5.txt\n",
        "!wc -l train5.txt\n",
        "# test\n",
        "!head -$(cat train4.txt | python -c 'import sys; print(round(len([line.strip() for line \\\n",
        "in sys.stdin]) * 0.25))') train4.txt > test5.txt\n",
        "!wc -l test5.txt\n",
        "# morfessor\n",
        "!morfessor -t train5.txt -T test5.txt >> krc_segmented.txt"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7918 train5.txt\n",
            "1980 test5.txt\n",
            "Reading corpus from 'train5.txt'...\n",
            "Detected utf-8 encoding\n",
            "Done.\n",
            "Compounds in training data: 7214 types / 7214 tokens\n",
            "Starting batch training\n",
            "Epochs: 0\tCost: 220694.61388327525\n",
            "100% (7214 of 7214) |####################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
            "Epochs: 1\tCost: 155717.83797325596\n",
            "100% (7214 of 7214) |####################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
            "Epochs: 2\tCost: 146160.0214953182\n",
            "100% (7214 of 7214) |####################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
            "Epochs: 3\tCost: 144700.08871557133\n",
            "100% (7214 of 7214) |####################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
            "Epochs: 4\tCost: 144413.2052952422\n",
            "100% (7214 of 7214) |####################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
            "Epochs: 5\tCost: 144237.28918812537\n",
            "100% (7214 of 7214) |####################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
            "Epochs: 6\tCost: 144189.21061867342\n",
            "100% (7214 of 7214) |####################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
            "Epochs: 7\tCost: 144189.1943159561\n",
            "Done.\n",
            "Epochs: 7\n",
            "Final cost: 144189.1943159561\n",
            "Training time: 31.576s\n",
            "Segmenting test data...\n",
            "Reading corpus from 'test5.txt'...\n",
            "Done.\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe_3D1F-3qoc",
        "colab_type": "text"
      },
      "source": [
        "### Download morfessor segmentation file if needed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljXkxC16n6M7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://www.dropbox.com/s/j9m33ajh77aec0j/krc_segmented.txt?dl=0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJ2i6P3W6D1S",
        "colab_type": "code",
        "outputId": "856d9ddb-deeb-4946-ae32-1cc5447ade5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "!wc -l krc_segmented.txt?dl=0"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9900 krc_segmented.txt?dl=0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDIRrupM2BZc",
        "colab_type": "code",
        "outputId": "b39c13c6-65fa-4e20-bcbc-b3c9117c8d6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "source": [
        "!head krc_segmented.txt"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "обур ланы\n",
            "област ына\n",
            "област ланы\n",
            "о аз и с ле\n",
            "нюзюр леринде\n",
            "нюзюр лери\n",
            "нохта га\n",
            "норма ланы\n",
            "норма га\n",
            "номер инден\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVn5JtQ9kQFz",
        "colab_type": "text"
      },
      "source": [
        "# Morphnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nU_nm8iRkOkj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_ipython().system(f\"git clone https://github.com/NIS-2018-CROSS-M/colab-tools.git\")\n",
        "get_ipython().magic(f\"cd colab-tools\")\n",
        "get_ipython().system(f\"bash colab-install-opennmt.sh\")\n",
        "get_ipython().system(f\"bash colab-install-cuda92-pytorch41.sh\")\n",
        "get_ipython().system(f\"bash colab-install-torchtext.sh\")\n",
        "get_ipython().magic(f\"cd ..\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KXaHnCpkUFG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# install dependencies used in calma project\n",
        "get_ipython().system('/usr/bin/python3 -m pip install configargparse')\n",
        "get_ipython().system('git clone https://github.com/NIS-2018-CROSS-M/calma_tools.git')\n",
        "\n",
        "# receive the calma\n",
        "get_ipython().system('git clone https://github.com/NIS-2018-CROSS-M/vardial-shared-task')\n",
        "get_ipython().magic('cd vardial-shared-task')\n",
        "get_ipython().system('rm onmt-data/*')\n",
        "get_ipython().system('rm results/*')\n",
        "get_ipython().magic('cd ..')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9amb2axo7QX7",
        "colab_type": "text"
      },
      "source": [
        "# Tag projection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9_3PxZT7dup",
        "colab_type": "text"
      },
      "source": [
        "## trk and krc intersection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb0-a8D97oT2",
        "colab_type": "text"
      },
      "source": [
        "###  Creating trk dictionary; key:value  == wordform:[lemma, pos, analysis]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a1CWRMMJl2aW",
        "outputId": "c707334d-aee4-4eef-c653-7ce9d06b4c05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "from collections import defaultdict as dd\n",
        "\n",
        "\n",
        "trk_keys = [line.strip().split('\\t')[1] for line in open('trk-uncovered-transliterated', 'r', encoding='utf-8') if line]\n",
        "trk_values = [line.strip().split('\\t')[2:] for line in open('trk-uncovered-transliterated', 'r', encoding='utf-8') if\n",
        "              line]\n",
        "\n",
        "# trk dictionary with all the analyses given\n",
        "trk, trk_intersection = dict(zip(trk_keys, trk_values)), dd(set)\n",
        "\n",
        "for line in open('trk-uncovered-transliterated', 'r'):\n",
        "    iso, wf, lem, pos, analysis = line.rstrip().split('\\t')\n",
        "    trk_intersection[wf].add((lem, pos, analysis))\n",
        "\n",
        "print('trk:', list(trk.items())[:5])\n",
        "print('trk_intersection:', list(trk_intersection.items())[:5])\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trk: [('казак', ['казак', 'NOUN', 'Case=Nom']), ('муамеле', ['муамеле', 'NOUN', 'Case=Nom']), ('заманы', ['заман', 'NOUN', 'Case=Nom|Number[psor]=Sing,Plur|Person[psor]=3']), ('конут', ['конут', 'NOUN', 'Case=Nom']), ('корумасы', ['корума', 'NOUN', 'Case=Nom|Number[psor]=Sing,Plur|Person[psor]=3'])]\n",
            "trk_intersection: [('казак', {('казак', 'NOUN', 'Case=Nom'), ('казак', 'ADJ', '_')}), ('муамеле', {('муамеле', 'NOUN', 'Case=Nom')}), ('заманы', {('заман', 'NOUN', 'Case=Nom|Number[psor]=Sing,Plur|Person[psor]=3'), ('заман', 'NOUN', 'Case=Acc')}), ('конут', {('конут', 'NOUN', 'Case=Nom')}), ('корумасы', {('кору', 'VERB', 'Case=Nom|Number[psor]=Sing,Plur|Person[psor]=3|Valency=2|VerbForm=Vnoun'), ('корума', 'NOUN', 'Case=Nom|Number[psor]=Sing,Plur|Person[psor]=3')})]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcRbN_LC7UZM",
        "colab_type": "text"
      },
      "source": [
        "### Creating krc dictionary; wf_trk[i] tags are projected on wf_krc[i] within the intersection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QBn93CdkO1CH",
        "outputId": "c336d32b-c368-4c07-bbcf-4f6cd8fc6b36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "krc_words = [line.strip().split('\\t')[1] for line in open('trk-covered-transliterated', 'r', encoding='utf-8')]\n",
        "krc = {wordform: trk_intersection[wordform] for wordform in krc_words if wordform in trk_intersection.keys()}\n",
        "print('Number of words in the intersection: {}'.format(len(krc.keys())))\n",
        "print('Examples: {}'.format(list(krc.items())[:2]))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words in the intersection: 1763\n",
            "Examples: [('крал', {('крал', 'NOUN', 'Case=Nom')}), ('алай', {('алай', 'NOUN', 'Case=Nom')})]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eIC5QQWK4Ac",
        "colab_type": "text"
      },
      "source": [
        "## Reading segmented test wordforms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y4SnOHmnsBFX",
        "outputId": "ec86081f-de70-40f6-b030-50d80c320b1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "morfessor = [line.strip() for line in open('krc_segmented.txt?dl=0', 'r', encoding='utf-8')]\n",
        "morfessor[:5]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['обур ланы', 'област ына', 'област ланы', 'о аз и с ле', 'нюзюр леринде']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBxNRqMFLP8t",
        "colab_type": "text"
      },
      "source": [
        "###  Create a list of morfessor lemmas\n",
        "We consider the substring of index 0 as a morfessor lemma, e.g. in \"музыка да\" the lem_morf is \"музыка\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9302RIFmMMhq",
        "colab_type": "code",
        "outputId": "898c9165-a429-42f7-d7fe-002bdc77ed87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "morfessor_lemmas = [wordform.split()[0] for wordform in morfessor]\n",
        "morfessor_lemmas[:5]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['обур', 'област', 'област', 'о', 'нюзюр']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_CIl6-sJ0hB",
        "colab_type": "text"
      },
      "source": [
        "## NOUN cluster\n",
        "\n",
        "We consider the krc wordforms to be in one noun cluster if wf_krc[i] == lem_trk[i] and w_trk[i] POS is NOUN "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pR7hmFr6nDZo",
        "outputId": "9c2767c7-ea38-41fc-ec9b-a16e55d7435c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "krc_noun_lemmas = [wf for wf in krc.keys() if wf == trk[wf][0] and trk[wf][1] == 'NOUN']\n",
        "print('Number of krc noun lemmas: {}'.format(len(krc_noun_lemmas)))\n",
        "print('Examples: {}'.format(list(krc_noun_lemmas[:5])))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of krc noun lemmas: 912\n",
            "Examples: ['крал', 'алай', 'баш', 'адам', 'аскер']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sk_1mHViOU4E",
        "colab_type": "text"
      },
      "source": [
        "### Finding the intersection of krc noun lemmas and morfessor lemmas\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rFkPWE4-kT6Q",
        "outputId": "28a4e55b-45ae-4b82-b450-58fd8119c4fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "lemma_noun_intersection = set(krc_noun_lemmas) & set(morfessor_lemmas)\n",
        "print('Number of noun lemmas in the intersection: {}'.format(len(lemma_noun_intersection)))\n",
        "print('Examples: {}'.format(list(lemma_noun_intersection)[:5]))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of noun lemmas in the intersection: 481\n",
            "Examples: ['театр', 'функция', 'билим', 'кум', 'ал']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rhtqASQ0Cq2",
        "colab_type": "text"
      },
      "source": [
        "### If we can find morfessor lemma in lemma_noun_intersection, we keep the segmented wordform in noun_segmented"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5Bm50a5omYL2",
        "outputId": "a01b1b07-f9aa-4e34-85be-6f65f8e0a5bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "noun_segmented = [segmented for segmented in morfessor for lemma in lemma_noun_intersection \n",
        "                  if lemma == segmented.split()[0]]\n",
        "\n",
        "print('Number of words in the cluster: {}'.format(len(noun_segmented)))\n",
        "print('Examples: {}'.format(list(noun_segmented[:5])))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words in the cluster: 3127\n",
            "Examples: ['норма ланы', 'норма га', 'низам лары', 'низам ланы', 'низам ла']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYxsX4Hr-X3q",
        "colab_type": "text"
      },
      "source": [
        "### If morfessor wordform == train wordform and if morfessor lemma == train lemma, we project tags from the train file onto the morfessor wordform"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zt_Ydvr9zb3s",
        "outputId": "ba0ae204-e360-4d63-f3e0-532695844ac6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "noun_projections = [(segmented, trk[wordform]) for segmented in noun_segmented for wordform in\n",
        "                    trk.keys() if ''.join(segmented.split()) == wordform and trk[wordform][0] ==\n",
        "                    segmented.split()[0] and trk[wordform][1] == 'NOUN']\n",
        "noun_projections[:5]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('музыка да', ['музыка', 'NOUN', 'Case=Loc']),\n",
              " ('миф', ['миф', 'NOUN', 'Case=Nom']),\n",
              " ('миссия', ['миссия', 'NOUN', 'Case=Nom']),\n",
              " ('министр', ['министр', 'NOUN', 'Case=Nom']),\n",
              " ('мермер', ['мермер', 'NOUN', 'Case=Nom'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6QEQ_EEJfoaW",
        "outputId": "dbee3324-dffd-4f73-848a-cbd723d9d6d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "noun_morphemes = [projection[0].split()[1] for projection in noun_projections if\n",
        "                  ''.join(projection[0].split()) != projection[1][0]]\n",
        "noun_m_freq = {morpheme: noun_morphemes.count(morpheme) for morpheme in noun_morphemes}\n",
        "list(noun_m_freq.items())[:5]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('да', 25), ('ы', 36), ('лары', 33), ('ун', 7), ('сы', 37)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyGJjyARd1K6",
        "colab_type": "code",
        "outputId": "09309cdf-a539-4704-e461-7d6c6981224a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "noun_morphs = {projection[0].split()[1]: projection[1][2] for projection in noun_projections if projection[1][1]\n",
        "               == 'NOUN' and ''.join(projection[0].split()) != projection[1][0]}\n",
        "\n",
        "morphemes_2_tagsets = {key: set(value.split('|')) for key, value in noun_morphs.items()}\n",
        "\n",
        "inflected_morphemes_2_possible_tagsets = {}\n",
        "\n",
        "for morpheme1 in morphemes_2_tagsets.keys():\n",
        "    for morpheme2 in morphemes_2_tagsets.keys():\n",
        "\n",
        "        if morpheme1 == morpheme2 or not morpheme2.endswith(morpheme1):\n",
        "            continue\n",
        "\n",
        "        morpheme1_in_morpheme2_idx = morpheme2.find(morpheme1)\n",
        "        new_morpheme = morpheme2[:morpheme1_in_morpheme2_idx]\n",
        "\n",
        "        if len(new_morpheme) > 1:\n",
        "            new_morpheme_tagset = morphemes_2_tagsets[morpheme2] - morphemes_2_tagsets[morpheme1]\n",
        "\n",
        "            if new_morpheme not in inflected_morphemes_2_possible_tagsets.keys():\n",
        "                inflected_morphemes_2_possible_tagsets[new_morpheme] = []\n",
        "            inflected_morphemes_2_possible_tagsets[new_morpheme].append(new_morpheme_tagset)\n",
        "\n",
        "result_morphemes_2_possible_tagsets = {key: [value] for key, value in morphemes_2_tagsets.items()}\n",
        "for morpheme, possible_tagsets in inflected_morphemes_2_possible_tagsets.items():\n",
        "    for possible_tagset in possible_tagsets:\n",
        "\n",
        "        if morpheme not in result_morphemes_2_possible_tagsets.keys():\n",
        "            result_morphemes_2_possible_tagsets[morpheme] = [possible_tagset]\n",
        "\n",
        "        known_tagsets_of_morpheme = result_morphemes_2_possible_tagsets[morpheme]\n",
        "\n",
        "        if not all(tag in known_tagset for tag in possible_tagset for known_tagset in known_tagsets_of_morpheme):\n",
        "            result_morphemes_2_possible_tagsets[morpheme].append(possible_tagset)\n",
        "\n",
        "for morpheme, tagsets in result_morphemes_2_possible_tagsets.items():\n",
        "    if len(tagsets) > 1:\n",
        "        if len(tagsets) > 1:\n",
        "            if morpheme == 'ын':\n",
        "                del tagsets[0]\n",
        "            else:\n",
        "                del tagsets[1]\n",
        "\n",
        "noun_segment = {morpheme: known_tagsets[0]\n",
        "                for morpheme, known_tagsets in result_morphemes_2_possible_tagsets.items()}\n",
        "\n",
        "list(noun_segment.items())[:5]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('да', {'Case=Loc'}),\n",
              " ('ы', {'Case=Nom', 'Number[psor]=Sing,Plur', 'Person[psor]=3'}),\n",
              " ('лары',\n",
              "  {'Case=Nom', 'Number=Plur', 'Number[psor]=Sing,Plur', 'Person[psor]=3'}),\n",
              " ('ун', {'Case=Acc', 'Number[psor]=Sing,Plur', 'Person[psor]=3'}),\n",
              " ('сы', {'Case=Nom', 'Number[psor]=Sing,Plur', 'Person[psor]=3'})]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRK8Dje0M0WW",
        "colab_type": "text"
      },
      "source": [
        "## VERB cluster\n",
        "\n",
        "We consider the krc wordforms to be in one noun cluster if wf_krc[i] == lem_trk[i] and w_trk[i] POS is VERB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vpJKAagMzbm",
        "colab_type": "code",
        "outputId": "dd1863c4-6648-451a-d555-dc1f2f4eb720",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "krc_verb_lemmas = [wf for wf in krc.keys() if wf == trk[wf][0] and trk[wf][1] == 'VERB']\n",
        "len(krc_verb_lemmas)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "39"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdrA2S5gS50e",
        "colab_type": "code",
        "outputId": "f0a57c0d-ac25-41bc-fe90-8f1933f2680b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "lemma_verb_intersection = set(krc_verb_lemmas) & set(morfessor_lemmas)\n",
        "len(lemma_verb_intersection)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xc2wPKMEUKIB",
        "colab_type": "code",
        "outputId": "d97db9f5-3ac0-4e70-ea13-a32842ef7d6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "verb_segment = [segmented for segmented in morfessor for lemma in lemma_verb_intersection\n",
        "                if lemma == segmented.split()[0]]\n",
        "verb_segment[:5]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['кыз лары', 'кыз га', 'кул лугу ндан', 'кул лугу н', 'кул ак лары']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yU6WS2mBVopw",
        "colab_type": "code",
        "outputId": "0b90e603-27b7-44e2-dec0-75eaddedb292",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "verb_projections = [(segmented, trk[wordform]) for segmented in verb_segment for wordform in\n",
        "                    trk.keys() if ''.join(segmented.split()) == wordform and trk[wordform][0] ==\n",
        "                    segmented.split()[0] and trk[wordform][1] == 'VERB']\n",
        "verb_projections[:5]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('кара р',\n",
              "  ['кара', 'VERB', 'Number=Sing|Person=3|Tense=Aor|Valency=1|VerbForm=Fin']),\n",
              " ('кара п', ['кара', 'VERB', 'Aspect=Perf|Valency=1|VerbForm=Conv']),\n",
              " ('кир',\n",
              "  ['кир', 'VERB', 'Mood=Imp|Number=Sing|Person=2|Valency=1|VerbForm=Fin']),\n",
              " ('де се к',\n",
              "  ['де', 'VERB', 'Mood=Cond|Number=Plur|Person=1|Valency=2|VerbForm=Conv']),\n",
              " ('ат ылган',\n",
              "  ['ат',\n",
              "   'VERB',\n",
              "   'Number=Sing|Person=3|Tense=Past|Valency=2|VerbForm=Fin|Voice=Pass'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9Bbu4hXebJY",
        "colab_type": "code",
        "outputId": "e484f9bc-cae0-490f-c01e-6c99e6f326f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "verb_morphemes = [''.join(projection[0].split()[1:]) for projection in verb_projections\n",
        "                  if ''.join(projection[0].split()) != projection[1][0]]\n",
        "verb_m_freq = {morpheme: verb_morphemes.count(morpheme) for morpheme in verb_morphemes}\n",
        "list(verb_m_freq.items())[:5]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('р', 2), ('п', 3), ('сек', 2), ('ылган', 2), ('ип', 1)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DN-qm1d-ub6",
        "colab_type": "code",
        "outputId": "688b6790-4ff2-4c4d-f0b1-3b0c238cc0a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "verb_morphs = {projection[0].split()[1]: projection[1][2] for projection in verb_projections if projection[1][1]\n",
        "         == 'VERB' and ''.join(projection[0].split()) != projection[1][0]}\n",
        "\n",
        "morphemes_2_tagsets = {key: set(value.split('|')) for key, value in verb_morphs.items()}\n",
        "\n",
        "inflected_morphemes_2_possible_tagsets = {}\n",
        "\n",
        "for morpheme1 in morphemes_2_tagsets.keys():\n",
        "    for morpheme2 in morphemes_2_tagsets.keys():\n",
        "\n",
        "        if morpheme1 == morpheme2 or not morpheme2.endswith(morpheme1):\n",
        "            continue\n",
        "\n",
        "        morpheme1_in_morpheme2_idx = morpheme2.find(morpheme1)\n",
        "        new_morpheme = morpheme2[:morpheme1_in_morpheme2_idx]\n",
        "\n",
        "        if len(new_morpheme) > 1:\n",
        "            new_morpheme_tagset = morphemes_2_tagsets[morpheme2] - morphemes_2_tagsets[morpheme1]\n",
        "\n",
        "            if new_morpheme not in inflected_morphemes_2_possible_tagsets.keys():\n",
        "                inflected_morphemes_2_possible_tagsets[new_morpheme] = []\n",
        "            inflected_morphemes_2_possible_tagsets[new_morpheme].append(new_morpheme_tagset)\n",
        "\n",
        "\n",
        "result_morphemes_2_possible_tagsets = {key: [value] for key, value in morphemes_2_tagsets.items()}\n",
        "for morpheme, possible_tagsets in inflected_morphemes_2_possible_tagsets.items():\n",
        "    for possible_tagset in possible_tagsets:\n",
        "\n",
        "        if morpheme not in result_morphemes_2_possible_tagsets.keys():\n",
        "            result_morphemes_2_possible_tagsets[morpheme] = [possible_tagset]\n",
        "\n",
        "        known_tagsets_of_morpheme = result_morphemes_2_possible_tagsets[morpheme]\n",
        "\n",
        "        if not all(tag in known_tagset for tag in possible_tagset for known_tagset in known_tagsets_of_morpheme):\n",
        "            result_morphemes_2_possible_tagsets[morpheme].append(possible_tagset)\n",
        "\n",
        "for morpheme, tagsets in result_morphemes_2_possible_tagsets.items():\n",
        "    if len(tagsets) > 1:\n",
        "        print(f\"error! morpheme {morpheme} relates to more than one possible tags string\")\n",
        "\n",
        "verb_segment = {morpheme: known_tagsets[0]\n",
        "                              for morpheme, known_tagsets in result_morphemes_2_possible_tagsets.items()}\n",
        "\n",
        "list(verb_segment.items())[:5]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('р', {'Number=Sing', 'Person=3', 'Tense=Aor', 'Valency=1', 'VerbForm=Fin'}),\n",
              " ('п', {'Aspect=Perf', 'Valency=2', 'VerbForm=Conv'}),\n",
              " ('се',\n",
              "  {'Mood=Cond', 'Number=Plur', 'Person=3', 'Valency=1', 'VerbForm=Conv'}),\n",
              " ('ылган',\n",
              "  {'Number=Sing',\n",
              "   'Person=3',\n",
              "   'Tense=Past',\n",
              "   'Valency=2',\n",
              "   'VerbForm=Fin',\n",
              "   'Voice=Pass'}),\n",
              " ('и', {'Aspect=Perf', 'Valency=1', 'VerbForm=Conv'})]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bi-mHufC8Gio",
        "colab_type": "text"
      },
      "source": [
        "# Calma POS and my greedy segmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcL8x65A65XG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://www.dropbox.com/s/yy6w45r54rjj3ll/trk-test-covered.sys?dl=0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZExKtIy8LtI",
        "colab_type": "code",
        "outputId": "4fc47931-9737-4a13-dc0b-69381632f9bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "source": [
        "!head trk-test-covered.sys?dl=0"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "krc\tджыл\tcc\tNOUN\tTag1=Value1|Tag2=Value2\n",
            "krc\tджыл\tcc\tADJ\tTag1=Value1|Tag2=Value2\n",
            "krc\tэм\tcc\tNOUN\tTag1=Value1|Tag2=Value2\n",
            "krc\tэм\tcc\tVERB\tTag1=Value1|Tag2=Value2\n",
            "krc\tджылны\tcc\tNOUN\tTag1=Value1|Tag2=Value2\n",
            "krc\tболгъанды\tcc\tVERB\tTag1=Value1|Tag2=Value2\n",
            "krc\tдери\tcc\tNOUN\tTag1=Value1|Tag2=Value2\n",
            "krc\tдери\tcc\tADJ\tTag1=Value1|Tag2=Value2\n",
            "krc\tкёре\tcc\tNOUN\tTag1=Value1|Tag2=Value2\n",
            "krc\tкёре\tcc\tVERB\tTag1=Value1|Tag2=Value2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K81Adv_j8bw7",
        "colab_type": "code",
        "outputId": "15b594b7-d37a-4f24-ae2a-a06249ebcad4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "calma = pd.read_csv('trk-test-covered.sys?dl=0', sep='\\t', names=['iso', 'transliterated', 'lemma', 'POS', 'analysis'], index_col=False)\n",
        "calma.insert(1, 'wordform', calma['transliterated'])\n",
        "\n",
        "calma.head()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>iso</th>\n",
              "      <th>wordform</th>\n",
              "      <th>transliterated</th>\n",
              "      <th>lemma</th>\n",
              "      <th>POS</th>\n",
              "      <th>analysis</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>krc</td>\n",
              "      <td>джыл</td>\n",
              "      <td>джыл</td>\n",
              "      <td>cc</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>Tag1=Value1|Tag2=Value2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>krc</td>\n",
              "      <td>джыл</td>\n",
              "      <td>джыл</td>\n",
              "      <td>cc</td>\n",
              "      <td>ADJ</td>\n",
              "      <td>Tag1=Value1|Tag2=Value2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>krc</td>\n",
              "      <td>эм</td>\n",
              "      <td>эм</td>\n",
              "      <td>cc</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>Tag1=Value1|Tag2=Value2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>krc</td>\n",
              "      <td>эм</td>\n",
              "      <td>эм</td>\n",
              "      <td>cc</td>\n",
              "      <td>VERB</td>\n",
              "      <td>Tag1=Value1|Tag2=Value2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>krc</td>\n",
              "      <td>джылны</td>\n",
              "      <td>джылны</td>\n",
              "      <td>cc</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>Tag1=Value1|Tag2=Value2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   iso wordform transliterated lemma   POS                 analysis\n",
              "0  krc     джыл           джыл    cc  NOUN  Tag1=Value1|Tag2=Value2\n",
              "1  krc     джыл           джыл    cc   ADJ  Tag1=Value1|Tag2=Value2\n",
              "2  krc       эм             эм    cc  NOUN  Tag1=Value1|Tag2=Value2\n",
              "3  krc       эм             эм    cc  VERB  Tag1=Value1|Tag2=Value2\n",
              "4  krc   джылны         джылны    cc  NOUN  Tag1=Value1|Tag2=Value2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PG_5cR0-mj6",
        "colab_type": "code",
        "outputId": "5e8cce2c-2acb-45d6-fbe2-572bb808c95b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# creating three calma pos clusters\n",
        "\n",
        "clusters, wordforms = list(calma.groupby(['POS']).groups.keys()), []\n",
        "\n",
        "for cluster in clusters:\n",
        "  wordforms.append(calma.loc[calma.groupby(['POS']).groups[cluster],\n",
        "                            'transliterated'].tolist())\n",
        "  \n",
        "calma_pos = dict(zip(clusters, wordforms))\n",
        "\n",
        "for cluster in calma_pos.keys():\n",
        "  print((cluster, len(calma_pos[cluster])))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('ADJ', 702)\n",
            "('ADV', 266)\n",
            "('NOUN', 8897)\n",
            "('VERB', 2635)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "va9W1TUBAa50",
        "colab_type": "code",
        "outputId": "a1e9caf8-5288-4d97-9538-50531edf7a37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# NOUN\n",
        "\n",
        "input_wfs_set = set()\n",
        "greed_nouns = {}\n",
        "\n",
        "for noun in calma_pos['NOUN']:\n",
        "  input_wfs_set.add(noun)\n",
        "  \n",
        "processed_wfs_set = set()\n",
        "result_n_data = {}\n",
        "for morpheme in sorted(noun_segment.keys(), key=lambda word: len(word), reverse=True):\n",
        "    processed_wfs_set = set()\n",
        "    for wf in input_wfs_set:\n",
        "        if wf.endswith(morpheme):\n",
        "\n",
        "            if wf != morpheme:\n",
        "                lemma = wf[:wf.rfind(morpheme)]\n",
        "            else:\n",
        "                lemma = \"NONE\"\n",
        "\n",
        "            result_n_data[wf] = [lemma] + ['NOUN'] + list(noun_segment[morpheme])\n",
        "            processed_wfs_set.add(wf)\n",
        "    input_wfs_set -= processed_wfs_set\n",
        "\n",
        "    \n",
        "for morpheme, inflected_data in result_n_data.items():\n",
        "    lemma = inflected_data[0]\n",
        "    pos = inflected_data[1]\n",
        "    analysis = sorted(inflected_data[2:])\n",
        "    greed_nouns[morpheme] = [lemma, pos, '|'.join(analysis)]\n",
        "    \n",
        "for wf in input_wfs_set:\n",
        "    greed_nouns[wf] = [wf, 'NOUN', \"Case=Nom\"]\n",
        "                           \n",
        "len(list(greed_nouns.keys()))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7820"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_TEiPzVsxru",
        "colab_type": "code",
        "outputId": "2ca33ae6-50ae-4dc8-e429-bc1d56fad086",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "list(greed_nouns.items())[:2]"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('факторларындан',\n",
              "  ['фактор',\n",
              "   'NOUN',\n",
              "   'Case=Abl|Number=Plur|Number[psor]=Sing,Plur|Person[psor]=3']),\n",
              " ('районларындан',\n",
              "  ['район',\n",
              "   'NOUN',\n",
              "   'Case=Abl|Number=Plur|Number[psor]=Sing,Plur|Person[psor]=3'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9U1AO67USgg",
        "colab_type": "code",
        "outputId": "0346927a-c093-457b-f903-f89e057744eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# VERB\n",
        "\n",
        "input_wfs_set = set()\n",
        "greed_verbs = {}\n",
        "\n",
        "for verb in calma_pos['VERB']:\n",
        "  input_wfs_set.add(verb)\n",
        "\n",
        "processed_wfs_set = set()\n",
        "result_v_data = {}\n",
        "for morpheme in sorted(verb_segment.keys(), key=lambda word: len(word), reverse=True):\n",
        "    processed_wfs_set = set()\n",
        "    for wf in input_wfs_set:\n",
        "        if wf.endswith(morpheme):\n",
        "\n",
        "            if wf != morpheme:\n",
        "                lemma = wf[:wf.rfind(morpheme)]\n",
        "            else:\n",
        "                lemma = \"NONE\"\n",
        "\n",
        "            result_v_data[wf] = [lemma] + ['VERB'] + list(verb_segment[morpheme])\n",
        "            processed_wfs_set.add(wf)\n",
        "    input_wfs_set -= processed_wfs_set\n",
        "\n",
        "    \n",
        "for morpheme, inflected_data in result_v_data.items():\n",
        "    lemma = inflected_data[0]\n",
        "    pos = inflected_data[1]\n",
        "    analysis = sorted(inflected_data[2:])\n",
        "    greed_verbs[morpheme] = [lemma, pos, '|'.join(analysis)]\n",
        "    \n",
        "for wf in input_wfs_set:\n",
        "    greed_verbs[wf] = ['bad_verb', 'bad_verb', 'bad_verb']\n",
        "                           \n",
        "len(list(greed_verbs.keys()))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2391"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoLYdNz7W7Sa",
        "colab_type": "code",
        "outputId": "8c7abc61-4786-4ab3-e150-be7375b9f04e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "list(greed_verbs.items())[:2]"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('аллына',\n",
              "  ['алл', 'VERB', 'Number=Sing|Person=3|Tense=Pres|Valency=2|Voice=Pass']),\n",
              " ('тангына',\n",
              "  ['танг', 'VERB', 'Number=Sing|Person=3|Tense=Pres|Valency=2|Voice=Pass'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tLgGVbzzt0u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "419e6847-7d2e-4f1a-f47b-71f2e7e1e0a3"
      },
      "source": [
        "# Rules for derivational morphemes and Degree=Comp morphemes in ADJ cluster\n",
        "\n",
        "adj_segment = {\n",
        "    \"ирек\": \"Degree=Comp\",\n",
        "    \"ырак\": \"Degree=Comp\",\n",
        "    \"урак\": \"Degree=Comp\",\n",
        "    \"рек\": \"Degree=Comp\",\n",
        "    \"рак\": \"Degree=Comp\",\n",
        "    \"ду\": \"_\",\n",
        "    \"ды\": \"_\",\n",
        "    \"ди\": \"_\"\n",
        "}\n",
        "\n",
        "input_wfs_set = set()\n",
        "greed_adjs = {}\n",
        "\n",
        "for adj in calma_pos['ADJ']:\n",
        "  input_wfs_set.add(adj)\n",
        "\n",
        "processed_wfs_set = set()\n",
        "result_v_data = {}\n",
        "for morpheme in sorted(adj_segment.keys(), key=lambda word: len(word), reverse=True):\n",
        "    processed_wfs_set = set()\n",
        "    for wf in input_wfs_set:\n",
        "        if wf.endswith(morpheme):\n",
        "\n",
        "            if wf != morpheme:\n",
        "                lemma = wf[:wf.rfind(morpheme)]\n",
        "            else:\n",
        "                lemma = \"NONE\"\n",
        "\n",
        "            result_v_data[wf] = [lemma] + ['ADJ'] + list(adj_segment[morpheme])\n",
        "            processed_wfs_set.add(wf)\n",
        "    input_wfs_set -= processed_wfs_set\n",
        "\n",
        "    \n",
        "for morpheme, inflected_data in result_v_data.items():\n",
        "    lemma = inflected_data[0]\n",
        "    pos = inflected_data[1]\n",
        "    analysis = inflected_data[2:]\n",
        "    greed_adjs[morpheme] = [lemma, pos, ''.join(analysis)]\n",
        "    \n",
        "for wf in input_wfs_set:\n",
        "    greed_verbs[wf] = [wf, 'ADJ', '_']\n",
        "    \n",
        "len(list(greed_adjs.keys()))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "75"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ViOxYU01bA_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7e28075a-e9e0-43c1-9992-64fd99d91a12"
      },
      "source": [
        "list(greed_adjs.items())[:2]"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('кечирек', ['кеч', 'ADJ', 'Degree=Comp']),\n",
              " ('эскирек', ['эск', 'ADJ', 'Degree=Comp'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbD7VA4W3JcU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "calma_list = []\n",
        "\n",
        "unique_nouns, unique_verbs  = set(greed_nouns.keys()), set(greed_verbs.keys())\n",
        "unique_advs, unique_adjs = set(calma_pos['ADV']), set(greed_adjs.keys())\n",
        "\n",
        "\n",
        "with open('trk-test-covered.sys?dl=0', 'r') as f:\n",
        "  for line in f:\n",
        "    iso, wf, lemma, pos, analysis = line.strip().split('\\t')\n",
        "\n",
        "    if wf in unique_nouns and pos == greed_nouns[wf][1]:\n",
        "      lemma = greed_nouns[wf][0]\n",
        "      analysis = greed_nouns[wf][2]\n",
        "    else:\n",
        "      lemma = 'bad_noun'\n",
        "      analysis = 'bad_noun'\n",
        "      if wf in unique_verbs and pos == greed_verbs[wf][1]:\n",
        "        lemma = greed_verbs[wf][0]\n",
        "        analysis = greed_verbs[wf][2]\n",
        "      else:\n",
        "        lemma = 'bad_verb'\n",
        "        analysis = 'bad_verb'\n",
        "        if wf in unique_adjs and pos == greed_adjs[wf][1]:\n",
        "          lemma = greed_adjs[wf][0]\n",
        "          analysis = greed_adjs[wf][2]\n",
        "        elif wf in unique_advs and pos == \"ADV\":\n",
        "          lemma = wf\n",
        "          analysis = '_'\n",
        "    results=[]\n",
        "    \n",
        "    if wf in trk_intersection.keys() and wf in krc_words:\n",
        "      for trk_value in trk_intersection[wf]:\n",
        "        if pos == trk_value[1]:\n",
        "          results.append((trk_value[0], trk_value[2]))\n",
        "    if len(results) == 0:\n",
        "      results.append((lemma, analysis))\n",
        "    for result in results:\n",
        "      calma_list.append((iso, wf, result[0], pos, result[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3y-Fzk-4kYA",
        "colab_type": "code",
        "outputId": "05f8db98-1c29-4d41-b48f-f5f5189171a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(calma_list)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12922"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GnJoq8Qxx9j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "calm = pd.DataFrame(calma_list, columns=['iso', 'wf', 'lemma', 'pos', 'tags'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPEoQyOoOtrU",
        "colab_type": "code",
        "outputId": "19474303-859a-47b4-9924-bc9574f30ced",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "calm.head()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>iso</th>\n",
              "      <th>wf</th>\n",
              "      <th>lemma</th>\n",
              "      <th>pos</th>\n",
              "      <th>tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>krc</td>\n",
              "      <td>джыл</td>\n",
              "      <td>джыл</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>Case=Nom</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>krc</td>\n",
              "      <td>джыл</td>\n",
              "      <td>джыл</td>\n",
              "      <td>ADJ</td>\n",
              "      <td>_</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>krc</td>\n",
              "      <td>эм</td>\n",
              "      <td>эм</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>Case=Nom</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>krc</td>\n",
              "      <td>эм</td>\n",
              "      <td>bad_verb</td>\n",
              "      <td>VERB</td>\n",
              "      <td>bad_verb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>krc</td>\n",
              "      <td>джылны</td>\n",
              "      <td>джыл</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>Case=Acc</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   iso      wf     lemma   pos      tags\n",
              "0  krc    джыл      джыл  NOUN  Case=Nom\n",
              "1  krc    джыл      джыл   ADJ         _\n",
              "2  krc      эм        эм  NOUN  Case=Nom\n",
              "3  krc      эм  bad_verb  VERB  bad_verb\n",
              "4  krc  джылны      джыл  NOUN  Case=Acc"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ednjYlU0iXpw",
        "colab_type": "code",
        "outputId": "3c5e2d8d-9e1f-43a1-89b9-a4068d3da753",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "calm.tags.value_counts()[:5]"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Case=Nom                                          3433\n",
              "bad_verb                                          1708\n",
              "Case=Acc                                          1603\n",
              "Case=Nom|Number[psor]=Sing,Plur|Person[psor]=3    1170\n",
              "_                                                  957\n",
              "Name: tags, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaJtH9gY5BHF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bad_words = calm[calm.tags == 'bad_verb']\n",
        "bad_words.to_csv(\"bad_verbs.tsv\", sep=\"\\t\", header=False, index=False, encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPTrRXin5v1P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# predictions for bad words\n",
        "\n",
        "from collections import defaultdict as dd\n",
        "from random import shuffle\n",
        "import re\n",
        "import sys\n",
        "sys.path.append(get_ipython().getoutput(\"readlink -e calma_tools\")[0])\n",
        "from calma_tools.ml_util import MLUtil\n",
        "import urllib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oy7OEFJLni7u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_ipython().magic('cd vardial-shared-task')\n",
        "\n",
        "train_steps=10000\n",
        "valid_steps=1000\n",
        "save_checkpoint_steps = valid_steps\n",
        "\n",
        "train_params = [\n",
        "    f\"-train_steps {train_steps}\",\n",
        "    f\"-valid_steps {valid_steps}\",\n",
        "    f\"-save_checkpoint_steps {save_checkpoint_steps}\",\n",
        "    f\"-world_size 1\",\n",
        "    f\"-gpu_ranks 0 1\",\n",
        "    f\"-encoder_type brnn\"\n",
        "]\n",
        "\n",
        "pred_params = [\n",
        "    f\"-replace_unk\",\n",
        "    f\"-verbose\",\n",
        "    f\"-n_best 8\",\n",
        "    f\"-beam 8\"\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_sVl-aWnvuJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TrainDataModifyer:\n",
        "    @staticmethod\n",
        "    def modify_src_line(line):\n",
        "        return line\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def restore_src_line(line):\n",
        "        return line\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def modify_tgt_line(line):\n",
        "        return line\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def restore_tgt_line(line):\n",
        "        return line\n",
        "\n",
        "\n",
        "class NBestDataModifyer:\n",
        "    @staticmethod\n",
        "    def sent_to_baseline_compatible(line):\n",
        "        return line\n",
        "                       \n",
        "    @staticmethod\n",
        "    def hyp_to_baseline_compatible(line):\n",
        "        return line\n",
        "\n",
        "    \n",
        "class DataEvaluator:\n",
        "    otypes = [\"analysis\",\"lemma\",\"tag\"]\n",
        "    \n",
        "    @staticmethod\n",
        "    def update_data(data, line):\n",
        "        lan, wf, lemma, pos, msd = line.split('\\t')\n",
        "        \n",
        "        data[\"analysis\"][wf].add((lemma,pos,msd))\n",
        "        data[\"lemma\"][wf].add(lemma)\n",
        "        data[\"tag\"][wf].add((pos,msd))\n",
        "        \n",
        "        return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afCpUBarny5Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# filenames, many of them\n",
        "train_uncovered_filename = f\"train/trk-uncovered-transliterated\"\n",
        "train_res_src_filename = f\"onmt-data/trk-src-train.txt\"\n",
        "train_res_tgt_filename = f\"onmt-data/trk-tgt-train.txt\"\n",
        "\n",
        "test_covered_filename = f\"bad_verbs.tsv\"\n",
        "test_res_src_filename = f\"onmt-data/trk-src-test.txt\"\n",
        "test_res_tgt_filename = f\"onmt-data/trk-tgt-test.txt\"\n",
        "test_pred_output_filename = f\"results/tmp.txt\" # output :)\n",
        "\n",
        "val_covered_filename = f\"dev/trk-covered-transliterated\"\n",
        "val_uncovered_filename = f\"dev/trk-uncovered-transliterated\"\n",
        "val_res_src_filename = f\"onmt-data/trk-src-dev.txt\"\n",
        "val_res_tgt_filename = f\"onmt-data/trk-tgt-dev.txt\"\n",
        "val_pred_output_filename = f\"results/trk-dev-covered.sys\" # output :)\n",
        "\n",
        "\n",
        "model_filename = f\"models/trk.model\"\n",
        "\n",
        "score_log_filename = f\"trk-score.log\"\n",
        "\n",
        "def ml(train_params, prediction_params, dataModifyer, nbestModifyer, dataEvaluator):\n",
        "    mlUtil = MLUtil(prediction_params, dataModifyer, nbestModifyer)\n",
        "\n",
        "    get_ipython().system(f'touch {score_log_filename}')\n",
        "\n",
        "\n",
        "    # ml| data preprocessing\n",
        "    mlUtil.generate_data(train_uncovered_filename, train_res_src_filename, train_res_tgt_filename)\n",
        "    mlUtil.generate_data(val_uncovered_filename, val_res_src_filename, val_res_tgt_filename)\n",
        "    mlUtil.generate_data(test_covered_filename, test_res_src_filename, test_res_tgt_filename)\n",
        "\n",
        "    # ml| training\n",
        "    mlUtil.train(train_res_src_filename, train_res_tgt_filename, val_res_src_filename, val_res_tgt_filename, model_filename, train_params)\n",
        "\n",
        "\n",
        "    mlUtil.predict(model_filename, test_res_src_filename, test_covered_filename, test_pred_output_filename)\n",
        "\n",
        "\n",
        "    mlUtil.predict(model_filename, val_res_src_filename, val_covered_filename, val_pred_output_filename)\n",
        "\n",
        "    get_ipython().system(f'echo \"*===QUALITY ON VAL DATA===*\" >> {score_log_filename}')\n",
        "    mlUtil.score_predictions(val_pred_output_filename, val_uncovered_filename, score_log_filename, dataEvaluator)\n",
        "\n",
        "    # log eval results\n",
        "    get_ipython().system(f'cat {score_log_filename}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sz5ivlOoY9i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3516f06a-9569-44e9-d343-9a1f2a64903b"
      },
      "source": [
        "ml(train_params, pred_params, TrainDataModifyer, NBestDataModifyer, DataEvaluator)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2019-06-30 09:06:03,377 INFO] Extracting features...\n",
            "[2019-06-30 09:06:03,377 INFO]  * number of source features: 0.\n",
            "[2019-06-30 09:06:03,377 INFO]  * number of target features: 0.\n",
            "[2019-06-30 09:06:03,378 INFO] Building `Fields` object...\n",
            "[2019-06-30 09:06:03,378 INFO] Building & saving training data...\n",
            "[2019-06-30 09:06:03,378 INFO] Reading source and target files: onmt-data/trk-src-train.txt onmt-data/trk-tgt-train.txt.\n",
            "[2019-06-30 09:06:03,942 INFO] Building shard 0.\n",
            "[2019-06-30 09:06:05,181 INFO]  * saving 0th train data shard to models/trk.model-prepared_training_data.train.0.pt.\n",
            "[2019-06-30 09:06:07,324 INFO] Building & saving validation data...\n",
            "[2019-06-30 09:06:07,325 INFO] Reading source and target files: onmt-data/trk-src-dev.txt onmt-data/trk-tgt-dev.txt.\n",
            "[2019-06-30 09:06:07,334 INFO] Building shard 0.\n",
            "[2019-06-30 09:06:07,347 INFO]  * saving 0th valid data shard to models/trk.model-prepared_training_data.valid.0.pt.\n",
            "[2019-06-30 09:06:07,394 INFO] Building & saving vocabulary...\n",
            "[2019-06-30 09:06:08,103 INFO]  * reloading models/trk.model-prepared_training_data.train.0.pt.\n",
            "[2019-06-30 09:06:08,798 INFO]  * tgt vocab size: 112.\n",
            "[2019-06-30 09:06:08,799 INFO]  * src vocab size: 43.\n",
            "[2019-06-30 09:06:10,856 INFO]  * src vocab size = 43\n",
            "[2019-06-30 09:06:10,856 INFO]  * tgt vocab size = 112\n",
            "[2019-06-30 09:06:10,856 INFO] Building model...\n",
            "[2019-06-30 09:06:13,704 INFO] NMTModel(\n",
            "  (encoder): RNNEncoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(43, 500, padding_idx=1)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (rnn): LSTM(500, 250, num_layers=2, dropout=0.3, bidirectional=True)\n",
            "  )\n",
            "  (decoder): InputFeedRNNDecoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(112, 500, padding_idx=1)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.3)\n",
            "    (rnn): StackedLSTM(\n",
            "      (dropout): Dropout(p=0.3)\n",
            "      (layers): ModuleList(\n",
            "        (0): LSTMCell(1000, 500)\n",
            "        (1): LSTMCell(500, 500)\n",
            "      )\n",
            "    )\n",
            "    (attn): GlobalAttention(\n",
            "      (linear_in): Linear(in_features=500, out_features=500, bias=False)\n",
            "      (linear_out): Linear(in_features=1000, out_features=500, bias=False)\n",
            "    )\n",
            "  )\n",
            "  (generator): Sequential(\n",
            "    (0): Linear(in_features=500, out_features=112, bias=True)\n",
            "    (1): LogSoftmax()\n",
            "  )\n",
            ")\n",
            "[2019-06-30 09:06:13,705 INFO] encoder: 3029500\n",
            "[2019-06-30 09:06:13,705 INFO] decoder: 5870112\n",
            "[2019-06-30 09:06:13,705 INFO] * number of parameters: 8899612\n",
            "[2019-06-30 09:06:13,707 INFO] Starting training on GPU: [0, 1]\n",
            "[2019-06-30 09:06:13,707 INFO] Start training...\n",
            "[2019-06-30 09:06:14,433 INFO] Loading dataset from models/trk.model-prepared_training_data.train.0.pt, number of examples: 76665\n",
            "[2019-06-30 09:06:18,809 INFO] Step 50/10000; acc:   6.00; ppl: 278.28; xent: 5.63; lr: 1.00000;   0/7009 tok/s;      5 sec\n",
            "[2019-06-30 09:06:22,794 INFO] Step 100/10000; acc:  10.03; ppl: 155.44; xent: 5.05; lr: 1.00000;   0/9006 tok/s;      9 sec\n",
            "[2019-06-30 09:06:26,853 INFO] Step 150/10000; acc:  22.84; ppl: 27.12; xent: 3.30; lr: 1.00000;   0/8962 tok/s;     13 sec\n",
            "[2019-06-30 09:06:30,864 INFO] Step 200/10000; acc:  35.16; ppl: 11.32; xent: 2.43; lr: 1.00000;   0/8841 tok/s;     17 sec\n",
            "[2019-06-30 09:06:34,907 INFO] Step 250/10000; acc:  45.28; ppl:  6.76; xent: 1.91; lr: 1.00000;   0/8947 tok/s;     21 sec\n",
            "[2019-06-30 09:06:38,905 INFO] Step 300/10000; acc:  56.22; ppl:  4.35; xent: 1.47; lr: 1.00000;   0/8939 tok/s;     25 sec\n",
            "[2019-06-30 09:06:42,865 INFO] Step 350/10000; acc:  63.85; ppl:  3.33; xent: 1.20; lr: 1.00000;   0/8983 tok/s;     29 sec\n",
            "[2019-06-30 09:06:46,963 INFO] Step 400/10000; acc:  72.45; ppl:  2.42; xent: 0.88; lr: 1.00000;   0/8796 tok/s;     33 sec\n",
            "[2019-06-30 09:06:50,897 INFO] Step 450/10000; acc:  74.74; ppl:  2.26; xent: 0.81; lr: 1.00000;   0/9127 tok/s;     37 sec\n",
            "[2019-06-30 09:06:54,943 INFO] Step 500/10000; acc:  76.26; ppl:  2.14; xent: 0.76; lr: 1.00000;   0/8781 tok/s;     41 sec\n",
            "[2019-06-30 09:06:59,106 INFO] Step 550/10000; acc:  79.02; ppl:  1.93; xent: 0.66; lr: 1.00000;   0/8822 tok/s;     45 sec\n",
            "[2019-06-30 09:07:03,013 INFO] Step 600/10000; acc:  81.26; ppl:  1.76; xent: 0.56; lr: 1.00000;   0/8970 tok/s;     49 sec\n",
            "[2019-06-30 09:07:06,999 INFO] Step 650/10000; acc:  81.50; ppl:  1.77; xent: 0.57; lr: 1.00000;   0/8856 tok/s;     53 sec\n",
            "[2019-06-30 09:07:11,020 INFO] Step 700/10000; acc:  83.97; ppl:  1.60; xent: 0.47; lr: 1.00000;   0/9025 tok/s;     57 sec\n",
            "[2019-06-30 09:07:14,915 INFO] Step 750/10000; acc:  84.59; ppl:  1.59; xent: 0.47; lr: 1.00000;   0/9018 tok/s;     61 sec\n",
            "[2019-06-30 09:07:19,060 INFO] Step 800/10000; acc:  84.87; ppl:  1.57; xent: 0.45; lr: 1.00000;   0/8875 tok/s;     65 sec\n",
            "[2019-06-30 09:07:23,194 INFO] Step 850/10000; acc:  85.53; ppl:  1.53; xent: 0.43; lr: 1.00000;   0/8923 tok/s;     69 sec\n",
            "[2019-06-30 09:07:27,122 INFO] Step 900/10000; acc:  86.10; ppl:  1.48; xent: 0.39; lr: 1.00000;   0/8907 tok/s;     73 sec\n",
            "[2019-06-30 09:07:31,055 INFO] Step 950/10000; acc:  87.13; ppl:  1.45; xent: 0.37; lr: 1.00000;   0/8979 tok/s;     77 sec\n",
            "[2019-06-30 09:07:35,136 INFO] Step 1000/10000; acc:  86.87; ppl:  1.45; xent: 0.37; lr: 1.00000;   0/8939 tok/s;     81 sec\n",
            "[2019-06-30 09:07:35,144 INFO] Loading dataset from models/trk.model-prepared_training_data.valid.0.pt, number of examples: 1229\n",
            "[2019-06-30 09:07:36,754 INFO] Validation perplexity: 9.40163\n",
            "[2019-06-30 09:07:36,755 INFO] Validation accuracy: 73.7206\n",
            "[2019-06-30 09:07:36,755 INFO] Saving checkpoint models/trk.model_step_1000.pt\n",
            "[2019-06-30 09:07:40,792 INFO] Step 1050/10000; acc:  86.96; ppl:  1.45; xent: 0.37; lr: 1.00000;   0/6160 tok/s;     87 sec\n",
            "[2019-06-30 09:07:44,876 INFO] Step 1100/10000; acc:  87.88; ppl:  1.42; xent: 0.35; lr: 1.00000;   0/8962 tok/s;     91 sec\n",
            "[2019-06-30 09:07:48,907 INFO] Step 1150/10000; acc:  87.63; ppl:  1.41; xent: 0.35; lr: 1.00000;   0/8840 tok/s;     95 sec\n",
            "[2019-06-30 09:07:53,578 INFO] Loading dataset from models/trk.model-prepared_training_data.train.0.pt, number of examples: 76665\n",
            "[2019-06-30 09:07:53,893 INFO] Step 1200/10000; acc:  88.81; ppl:  1.35; xent: 0.30; lr: 1.00000;   0/7342 tok/s;    100 sec\n",
            "[2019-06-30 09:07:57,931 INFO] Step 1250/10000; acc:  88.35; ppl:  1.38; xent: 0.33; lr: 1.00000;   0/8817 tok/s;    104 sec\n",
            "[2019-06-30 09:08:01,907 INFO] Step 1300/10000; acc:  88.19; ppl:  1.38; xent: 0.32; lr: 1.00000;   0/9018 tok/s;    108 sec\n",
            "[2019-06-30 09:08:05,936 INFO] Step 1350/10000; acc:  88.72; ppl:  1.36; xent: 0.31; lr: 1.00000;   0/9058 tok/s;    112 sec\n",
            "[2019-06-30 09:08:09,973 INFO] Step 1400/10000; acc:  88.02; ppl:  1.38; xent: 0.32; lr: 1.00000;   0/8781 tok/s;    116 sec\n",
            "[2019-06-30 09:08:13,977 INFO] Step 1450/10000; acc:  89.07; ppl:  1.34; xent: 0.29; lr: 1.00000;   0/8975 tok/s;    120 sec\n",
            "[2019-06-30 09:08:18,019 INFO] Step 1500/10000; acc:  88.86; ppl:  1.35; xent: 0.30; lr: 1.00000;   0/8935 tok/s;    124 sec\n",
            "[2019-06-30 09:08:21,946 INFO] Step 1550/10000; acc:  88.85; ppl:  1.35; xent: 0.30; lr: 1.00000;   0/8994 tok/s;    128 sec\n",
            "[2019-06-30 09:08:26,027 INFO] Step 1600/10000; acc:  89.56; ppl:  1.33; xent: 0.28; lr: 1.00000;   0/8769 tok/s;    132 sec\n",
            "[2019-06-30 09:08:30,044 INFO] Step 1650/10000; acc:  89.63; ppl:  1.32; xent: 0.28; lr: 1.00000;   0/9052 tok/s;    136 sec\n",
            "[2019-06-30 09:08:34,063 INFO] Step 1700/10000; acc:  89.66; ppl:  1.32; xent: 0.28; lr: 1.00000;   0/8858 tok/s;    140 sec\n",
            "[2019-06-30 09:08:38,196 INFO] Step 1750/10000; acc:  89.78; ppl:  1.31; xent: 0.27; lr: 1.00000;   0/8852 tok/s;    144 sec\n",
            "[2019-06-30 09:08:42,152 INFO] Step 1800/10000; acc:  89.34; ppl:  1.32; xent: 0.28; lr: 1.00000;   0/8792 tok/s;    148 sec\n",
            "[2019-06-30 09:08:46,090 INFO] Step 1850/10000; acc:  89.53; ppl:  1.32; xent: 0.28; lr: 1.00000;   0/8935 tok/s;    152 sec\n",
            "[2019-06-30 09:08:50,154 INFO] Step 1900/10000; acc:  90.83; ppl:  1.27; xent: 0.24; lr: 1.00000;   0/8988 tok/s;    156 sec\n",
            "[2019-06-30 09:08:54,109 INFO] Step 1950/10000; acc:  89.94; ppl:  1.30; xent: 0.26; lr: 1.00000;   0/8935 tok/s;    160 sec\n",
            "[2019-06-30 09:08:58,257 INFO] Step 2000/10000; acc:  90.06; ppl:  1.30; xent: 0.26; lr: 1.00000;   0/8890 tok/s;    165 sec\n",
            "[2019-06-30 09:08:58,265 INFO] Loading dataset from models/trk.model-prepared_training_data.valid.0.pt, number of examples: 1229\n",
            "[2019-06-30 09:08:59,862 INFO] Validation perplexity: 8.60515\n",
            "[2019-06-30 09:08:59,862 INFO] Validation accuracy: 73.6564\n",
            "[2019-06-30 09:08:59,862 INFO] Saving checkpoint models/trk.model_step_2000.pt\n",
            "[2019-06-30 09:09:04,008 INFO] Step 2050/10000; acc:  90.47; ppl:  1.28; xent: 0.25; lr: 1.00000;   0/6373 tok/s;    170 sec\n",
            "[2019-06-30 09:09:08,023 INFO] Step 2100/10000; acc:  89.75; ppl:  1.30; xent: 0.26; lr: 1.00000;   0/8743 tok/s;    174 sec\n",
            "[2019-06-30 09:09:11,945 INFO] Step 2150/10000; acc:  90.55; ppl:  1.27; xent: 0.24; lr: 1.00000;   0/9050 tok/s;    178 sec\n",
            "[2019-06-30 09:09:16,036 INFO] Step 2200/10000; acc:  90.20; ppl:  1.29; xent: 0.26; lr: 1.00000;   0/8820 tok/s;    182 sec\n",
            "[2019-06-30 09:09:19,977 INFO] Step 2250/10000; acc:  90.30; ppl:  1.28; xent: 0.25; lr: 1.00000;   0/8911 tok/s;    186 sec\n",
            "[2019-06-30 09:09:24,062 INFO] Step 2300/10000; acc:  90.48; ppl:  1.28; xent: 0.25; lr: 1.00000;   0/8910 tok/s;    190 sec\n",
            "[2019-06-30 09:09:28,140 INFO] Step 2350/10000; acc:  90.23; ppl:  1.28; xent: 0.25; lr: 1.00000;   0/8814 tok/s;    194 sec\n",
            "[2019-06-30 09:09:32,584 INFO] Loading dataset from models/trk.model-prepared_training_data.train.0.pt, number of examples: 76665\n",
            "[2019-06-30 09:09:33,026 INFO] Step 2400/10000; acc:  91.12; ppl:  1.26; xent: 0.23; lr: 1.00000;   0/7405 tok/s;    199 sec\n",
            "[2019-06-30 09:09:37,094 INFO] Step 2450/10000; acc:  90.73; ppl:  1.27; xent: 0.24; lr: 1.00000;   0/8798 tok/s;    203 sec\n",
            "[2019-06-30 09:09:41,035 INFO] Step 2500/10000; acc:  90.10; ppl:  1.29; xent: 0.26; lr: 1.00000;   0/9004 tok/s;    207 sec\n",
            "[2019-06-30 09:09:45,112 INFO] Step 2550/10000; acc:  91.00; ppl:  1.26; xent: 0.23; lr: 1.00000;   0/9014 tok/s;    211 sec\n",
            "[2019-06-30 09:09:49,131 INFO] Step 2600/10000; acc:  90.10; ppl:  1.29; xent: 0.25; lr: 1.00000;   0/8835 tok/s;    215 sec\n",
            "[2019-06-30 09:09:53,161 INFO] Step 2650/10000; acc:  90.57; ppl:  1.27; xent: 0.24; lr: 1.00000;   0/8955 tok/s;    219 sec\n",
            "[2019-06-30 09:09:57,208 INFO] Step 2700/10000; acc:  90.43; ppl:  1.27; xent: 0.24; lr: 1.00000;   0/8952 tok/s;    224 sec\n",
            "[2019-06-30 09:10:01,123 INFO] Step 2750/10000; acc:  90.30; ppl:  1.28; xent: 0.25; lr: 1.00000;   0/8924 tok/s;    227 sec\n",
            "[2019-06-30 09:10:05,205 INFO] Step 2800/10000; acc:  91.12; ppl:  1.25; xent: 0.23; lr: 1.00000;   0/8795 tok/s;    231 sec\n",
            "[2019-06-30 09:10:09,195 INFO] Step 2850/10000; acc:  91.14; ppl:  1.25; xent: 0.23; lr: 1.00000;   0/9081 tok/s;    235 sec\n",
            "[2019-06-30 09:10:13,243 INFO] Step 2900/10000; acc:  90.92; ppl:  1.26; xent: 0.23; lr: 1.00000;   0/8821 tok/s;    240 sec\n",
            "[2019-06-30 09:10:17,385 INFO] Step 2950/10000; acc:  91.03; ppl:  1.26; xent: 0.23; lr: 1.00000;   0/8880 tok/s;    244 sec\n",
            "[2019-06-30 09:10:21,287 INFO] Step 3000/10000; acc:  90.37; ppl:  1.27; xent: 0.24; lr: 1.00000;   0/8834 tok/s;    248 sec\n",
            "[2019-06-30 09:10:21,294 INFO] Loading dataset from models/trk.model-prepared_training_data.valid.0.pt, number of examples: 1229\n",
            "[2019-06-30 09:10:22,901 INFO] Validation perplexity: 8.33448\n",
            "[2019-06-30 09:10:22,901 INFO] Validation accuracy: 75.1766\n",
            "[2019-06-30 09:10:22,902 INFO] Saving checkpoint models/trk.model_step_3000.pt\n",
            "[2019-06-30 09:10:26,936 INFO] Step 3050/10000; acc:  90.90; ppl:  1.26; xent: 0.23; lr: 1.00000;   0/6276 tok/s;    253 sec\n",
            "[2019-06-30 09:10:31,059 INFO] Step 3100/10000; acc:  91.90; ppl:  1.23; xent: 0.21; lr: 1.00000;   0/8911 tok/s;    257 sec\n",
            "[2019-06-30 09:10:34,946 INFO] Step 3150/10000; acc:  90.94; ppl:  1.25; xent: 0.23; lr: 1.00000;   0/8988 tok/s;    261 sec\n",
            "[2019-06-30 09:10:39,121 INFO] Step 3200/10000; acc:  91.09; ppl:  1.25; xent: 0.22; lr: 1.00000;   0/8862 tok/s;    265 sec\n",
            "[2019-06-30 09:10:43,214 INFO] Step 3250/10000; acc:  91.43; ppl:  1.24; xent: 0.21; lr: 1.00000;   0/8920 tok/s;    270 sec\n",
            "[2019-06-30 09:10:47,183 INFO] Step 3300/10000; acc:  90.89; ppl:  1.26; xent: 0.23; lr: 1.00000;   0/8867 tok/s;    273 sec\n",
            "[2019-06-30 09:10:51,174 INFO] Step 3350/10000; acc:  91.72; ppl:  1.23; xent: 0.20; lr: 1.00000;   0/8974 tok/s;    277 sec\n",
            "[2019-06-30 09:10:55,248 INFO] Step 3400/10000; acc:  90.98; ppl:  1.26; xent: 0.23; lr: 1.00000;   0/8747 tok/s;    282 sec\n",
            "[2019-06-30 09:10:59,155 INFO] Step 3450/10000; acc:  91.06; ppl:  1.25; xent: 0.22; lr: 1.00000;   0/9005 tok/s;    285 sec\n",
            "[2019-06-30 09:11:03,272 INFO] Step 3500/10000; acc:  91.57; ppl:  1.23; xent: 0.21; lr: 1.00000;   0/8961 tok/s;    290 sec\n",
            "[2019-06-30 09:11:07,367 INFO] Step 3550/10000; acc:  91.03; ppl:  1.25; xent: 0.22; lr: 1.00000;   0/8724 tok/s;    294 sec\n",
            "[2019-06-30 09:11:11,689 INFO] Loading dataset from models/trk.model-prepared_training_data.train.0.pt, number of examples: 76665\n",
            "[2019-06-30 09:11:12,299 INFO] Step 3600/10000; acc:  91.54; ppl:  1.24; xent: 0.22; lr: 1.00000;   0/7323 tok/s;    299 sec\n",
            "[2019-06-30 09:11:16,393 INFO] Step 3650/10000; acc:  91.71; ppl:  1.23; xent: 0.21; lr: 1.00000;   0/8731 tok/s;    303 sec\n",
            "[2019-06-30 09:11:20,342 INFO] Step 3700/10000; acc:  90.64; ppl:  1.26; xent: 0.23; lr: 1.00000;   0/8950 tok/s;    307 sec\n",
            "[2019-06-30 09:11:24,419 INFO] Step 3750/10000; acc:  91.77; ppl:  1.23; xent: 0.20; lr: 1.00000;   0/9071 tok/s;    311 sec\n",
            "[2019-06-30 09:11:28,406 INFO] Step 3800/10000; acc:  91.15; ppl:  1.24; xent: 0.22; lr: 1.00000;   0/8778 tok/s;    315 sec\n",
            "[2019-06-30 09:11:32,455 INFO] Step 3850/10000; acc:  91.76; ppl:  1.23; xent: 0.21; lr: 1.00000;   0/8988 tok/s;    319 sec\n",
            "[2019-06-30 09:11:36,510 INFO] Step 3900/10000; acc:  90.94; ppl:  1.25; xent: 0.22; lr: 1.00000;   0/8908 tok/s;    323 sec\n",
            "[2019-06-30 09:11:40,441 INFO] Step 3950/10000; acc:  91.08; ppl:  1.25; xent: 0.22; lr: 1.00000;   0/8927 tok/s;    327 sec\n",
            "[2019-06-30 09:11:44,492 INFO] Step 4000/10000; acc:  91.80; ppl:  1.22; xent: 0.20; lr: 1.00000;   0/8873 tok/s;    331 sec\n",
            "[2019-06-30 09:11:44,499 INFO] Loading dataset from models/trk.model-prepared_training_data.valid.0.pt, number of examples: 1229\n",
            "[2019-06-30 09:11:46,107 INFO] Validation perplexity: 9.56592\n",
            "[2019-06-30 09:11:46,107 INFO] Validation accuracy: 74.8626\n",
            "[2019-06-30 09:11:46,107 INFO] Saving checkpoint models/trk.model_step_4000.pt\n",
            "[2019-06-30 09:11:50,124 INFO] Step 4050/10000; acc:  91.92; ppl:  1.22; xent: 0.20; lr: 1.00000;   0/6396 tok/s;    336 sec\n",
            "[2019-06-30 09:11:54,160 INFO] Step 4100/10000; acc:  91.53; ppl:  1.23; xent: 0.20; lr: 1.00000;   0/8885 tok/s;    340 sec\n",
            "[2019-06-30 09:11:58,283 INFO] Step 4150/10000; acc:  91.86; ppl:  1.22; xent: 0.20; lr: 1.00000;   0/8853 tok/s;    345 sec\n",
            "[2019-06-30 09:12:02,248 INFO] Step 4200/10000; acc:  91.11; ppl:  1.24; xent: 0.22; lr: 1.00000;   0/8785 tok/s;    349 sec\n",
            "[2019-06-30 09:12:06,220 INFO] Step 4250/10000; acc:  91.28; ppl:  1.24; xent: 0.21; lr: 1.00000;   0/8894 tok/s;    353 sec\n",
            "[2019-06-30 09:12:10,369 INFO] Step 4300/10000; acc:  92.46; ppl:  1.21; xent: 0.19; lr: 1.00000;   0/8949 tok/s;    357 sec\n",
            "[2019-06-30 09:12:14,224 INFO] Step 4350/10000; acc:  91.35; ppl:  1.23; xent: 0.21; lr: 1.00000;   0/8958 tok/s;    361 sec\n",
            "[2019-06-30 09:12:18,451 INFO] Step 4400/10000; acc:  91.81; ppl:  1.22; xent: 0.20; lr: 1.00000;   0/8791 tok/s;    365 sec\n",
            "[2019-06-30 09:12:22,527 INFO] Step 4450/10000; acc:  91.77; ppl:  1.22; xent: 0.20; lr: 1.00000;   0/8986 tok/s;    369 sec\n",
            "[2019-06-30 09:12:26,482 INFO] Step 4500/10000; acc:  91.46; ppl:  1.23; xent: 0.21; lr: 1.00000;   0/8847 tok/s;    373 sec\n",
            "[2019-06-30 09:12:30,485 INFO] Step 4550/10000; acc:  92.14; ppl:  1.21; xent: 0.19; lr: 1.00000;   0/8971 tok/s;    377 sec\n",
            "[2019-06-30 09:12:34,469 INFO] Step 4600/10000; acc:  91.21; ppl:  1.25; xent: 0.22; lr: 1.00000;   0/8788 tok/s;    381 sec\n",
            "[2019-06-30 09:12:38,432 INFO] Step 4650/10000; acc:  91.83; ppl:  1.22; xent: 0.20; lr: 1.00000;   0/8927 tok/s;    385 sec\n",
            "[2019-06-30 09:12:42,563 INFO] Step 4700/10000; acc:  92.10; ppl:  1.21; xent: 0.19; lr: 1.00000;   0/9000 tok/s;    389 sec\n",
            "[2019-06-30 09:12:46,652 INFO] Step 4750/10000; acc:  91.84; ppl:  1.22; xent: 0.20; lr: 1.00000;   0/8755 tok/s;    393 sec\n",
            "[2019-06-30 09:12:50,765 INFO] Loading dataset from models/trk.model-prepared_training_data.train.0.pt, number of examples: 76665\n",
            "[2019-06-30 09:12:51,634 INFO] Step 4800/10000; acc:  92.47; ppl:  1.21; xent: 0.19; lr: 1.00000;   0/7329 tok/s;    398 sec\n",
            "[2019-06-30 09:12:55,636 INFO] Step 4850/10000; acc:  92.14; ppl:  1.21; xent: 0.19; lr: 1.00000;   0/8783 tok/s;    402 sec\n",
            "[2019-06-30 09:12:59,583 INFO] Step 4900/10000; acc:  91.43; ppl:  1.24; xent: 0.21; lr: 1.00000;   0/9002 tok/s;    406 sec\n",
            "[2019-06-30 09:13:03,650 INFO] Step 4950/10000; acc:  92.15; ppl:  1.21; xent: 0.19; lr: 1.00000;   0/9078 tok/s;    410 sec\n",
            "[2019-06-30 09:13:07,677 INFO] Step 5000/10000; acc:  91.66; ppl:  1.22; xent: 0.20; lr: 1.00000;   0/8738 tok/s;    414 sec\n",
            "[2019-06-30 09:13:07,684 INFO] Loading dataset from models/trk.model-prepared_training_data.valid.0.pt, number of examples: 1229\n",
            "[2019-06-30 09:13:09,281 INFO] Validation perplexity: 10.5641\n",
            "[2019-06-30 09:13:09,281 INFO] Validation accuracy: 75.1838\n",
            "[2019-06-30 09:13:09,281 INFO] Saving checkpoint models/trk.model_step_5000.pt\n",
            "[2019-06-30 09:13:13,394 INFO] Step 5050/10000; acc:  91.88; ppl:  1.21; xent: 0.19; lr: 1.00000;   0/6353 tok/s;    420 sec\n",
            "[2019-06-30 09:13:17,406 INFO] Step 5100/10000; acc:  91.16; ppl:  1.23; xent: 0.21; lr: 1.00000;   0/8884 tok/s;    424 sec\n",
            "[2019-06-30 09:13:21,372 INFO] Step 5150/10000; acc:  91.44; ppl:  1.23; xent: 0.21; lr: 1.00000;   0/8879 tok/s;    428 sec\n",
            "[2019-06-30 09:13:25,475 INFO] Step 5200/10000; acc:  92.21; ppl:  1.20; xent: 0.19; lr: 1.00000;   0/8847 tok/s;    432 sec\n",
            "[2019-06-30 09:13:29,454 INFO] Step 5250/10000; acc:  92.04; ppl:  1.21; xent: 0.19; lr: 1.00000;   0/9050 tok/s;    436 sec\n",
            "[2019-06-30 09:13:33,533 INFO] Step 5300/10000; acc:  91.98; ppl:  1.22; xent: 0.20; lr: 1.00000;   0/8789 tok/s;    440 sec\n",
            "[2019-06-30 09:13:37,673 INFO] Step 5350/10000; acc:  92.20; ppl:  1.21; xent: 0.19; lr: 1.00000;   0/8768 tok/s;    444 sec\n",
            "[2019-06-30 09:13:41,589 INFO] Step 5400/10000; acc:  91.41; ppl:  1.23; xent: 0.21; lr: 1.00000;   0/8844 tok/s;    448 sec\n",
            "[2019-06-30 09:13:45,575 INFO] Step 5450/10000; acc:  91.91; ppl:  1.22; xent: 0.20; lr: 1.00000;   0/8925 tok/s;    452 sec\n",
            "[2019-06-30 09:13:49,713 INFO] Step 5500/10000; acc:  92.89; ppl:  1.19; xent: 0.17; lr: 1.00000;   0/8968 tok/s;    456 sec\n",
            "[2019-06-30 09:13:53,590 INFO] Step 5550/10000; acc:  91.74; ppl:  1.23; xent: 0.20; lr: 1.00000;   0/8932 tok/s;    460 sec\n",
            "[2019-06-30 09:13:57,878 INFO] Step 5600/10000; acc:  92.33; ppl:  1.21; xent: 0.19; lr: 1.00000;   0/8686 tok/s;    464 sec\n",
            "[2019-06-30 09:14:01,918 INFO] Step 5650/10000; acc:  92.00; ppl:  1.21; xent: 0.19; lr: 1.00000;   0/9049 tok/s;    468 sec\n",
            "[2019-06-30 09:14:05,839 INFO] Step 5700/10000; acc:  91.84; ppl:  1.22; xent: 0.20; lr: 1.00000;   0/8850 tok/s;    472 sec\n",
            "[2019-06-30 09:14:09,889 INFO] Step 5750/10000; acc:  92.71; ppl:  1.19; xent: 0.17; lr: 1.00000;   0/8950 tok/s;    476 sec\n",
            "[2019-06-30 09:14:13,868 INFO] Step 5800/10000; acc:  91.63; ppl:  1.22; xent: 0.20; lr: 1.00000;   0/8757 tok/s;    480 sec\n",
            "[2019-06-30 09:14:17,820 INFO] Step 5850/10000; acc:  91.89; ppl:  1.21; xent: 0.19; lr: 1.00000;   0/8916 tok/s;    484 sec\n",
            "[2019-06-30 09:14:21,978 INFO] Step 5900/10000; acc:  92.76; ppl:  1.19; xent: 0.17; lr: 1.00000;   0/8985 tok/s;    488 sec\n",
            "[2019-06-30 09:14:26,032 INFO] Step 5950/10000; acc:  92.28; ppl:  1.21; xent: 0.19; lr: 1.00000;   0/8844 tok/s;    492 sec\n",
            "[2019-06-30 09:14:29,987 INFO] Loading dataset from models/trk.model-prepared_training_data.train.0.pt, number of examples: 76665\n",
            "[2019-06-30 09:14:31,017 INFO] Step 6000/10000; acc:  92.63; ppl:  1.19; xent: 0.18; lr: 1.00000;   0/7371 tok/s;    497 sec\n",
            "[2019-06-30 09:14:31,027 INFO] Loading dataset from models/trk.model-prepared_training_data.valid.0.pt, number of examples: 1229\n",
            "[2019-06-30 09:14:32,681 INFO] Validation perplexity: 9.80618\n",
            "[2019-06-30 09:14:32,681 INFO] Validation accuracy: 74.5843\n",
            "[2019-06-30 09:14:32,681 INFO] Saving checkpoint models/trk.model_step_6000.pt\n",
            "[2019-06-30 09:14:36,682 INFO] Step 6050/10000; acc:  92.30; ppl:  1.20; xent: 0.18; lr: 1.00000;   0/6152 tok/s;    503 sec\n",
            "[2019-06-30 09:14:40,626 INFO] Step 6100/10000; acc:  91.62; ppl:  1.23; xent: 0.20; lr: 1.00000;   0/9007 tok/s;    507 sec\n",
            "[2019-06-30 09:14:44,722 INFO] Step 6150/10000; acc:  92.51; ppl:  1.19; xent: 0.18; lr: 1.00000;   0/9053 tok/s;    511 sec\n",
            "[2019-06-30 09:14:48,737 INFO] Step 6200/10000; acc:  92.22; ppl:  1.20; xent: 0.18; lr: 1.00000;   0/8812 tok/s;    515 sec\n",
            "[2019-06-30 09:14:52,754 INFO] Step 6250/10000; acc:  92.37; ppl:  1.20; xent: 0.18; lr: 1.00000;   0/8955 tok/s;    519 sec\n",
            "[2019-06-30 09:14:56,800 INFO] Step 6300/10000; acc:  91.82; ppl:  1.22; xent: 0.20; lr: 1.00000;   0/8911 tok/s;    523 sec\n",
            "[2019-06-30 09:15:00,724 INFO] Step 6350/10000; acc:  91.84; ppl:  1.22; xent: 0.20; lr: 1.00000;   0/8888 tok/s;    527 sec\n",
            "[2019-06-30 09:15:04,817 INFO] Step 6400/10000; acc:  92.67; ppl:  1.19; xent: 0.17; lr: 1.00000;   0/8861 tok/s;    531 sec\n",
            "[2019-06-30 09:15:08,803 INFO] Step 6450/10000; acc:  92.42; ppl:  1.20; xent: 0.18; lr: 1.00000;   0/8968 tok/s;    535 sec\n",
            "[2019-06-30 09:15:12,845 INFO] Step 6500/10000; acc:  92.30; ppl:  1.20; xent: 0.18; lr: 1.00000;   0/8884 tok/s;    539 sec\n",
            "[2019-06-30 09:15:17,024 INFO] Step 6550/10000; acc:  92.46; ppl:  1.19; xent: 0.18; lr: 1.00000;   0/8723 tok/s;    543 sec\n",
            "[2019-06-30 09:15:20,938 INFO] Step 6600/10000; acc:  91.69; ppl:  1.22; xent: 0.20; lr: 1.00000;   0/8818 tok/s;    547 sec\n",
            "[2019-06-30 09:15:24,990 INFO] Step 6650/10000; acc:  92.60; ppl:  1.20; xent: 0.18; lr: 1.00000;   0/8886 tok/s;    551 sec\n",
            "[2019-06-30 09:15:29,128 INFO] Step 6700/10000; acc:  93.20; ppl:  1.17; xent: 0.16; lr: 1.00000;   0/8902 tok/s;    555 sec\n",
            "[2019-06-30 09:15:32,985 INFO] Step 6750/10000; acc:  92.17; ppl:  1.21; xent: 0.19; lr: 1.00000;   0/9063 tok/s;    559 sec\n",
            "[2019-06-30 09:15:37,269 INFO] Step 6800/10000; acc:  92.62; ppl:  1.19; xent: 0.17; lr: 1.00000;   0/8672 tok/s;    564 sec\n",
            "[2019-06-30 09:15:41,263 INFO] Step 6850/10000; acc:  92.25; ppl:  1.19; xent: 0.18; lr: 1.00000;   0/9101 tok/s;    568 sec\n",
            "[2019-06-30 09:15:45,172 INFO] Step 6900/10000; acc:  92.25; ppl:  1.20; xent: 0.19; lr: 1.00000;   0/8832 tok/s;    571 sec\n",
            "[2019-06-30 09:15:49,256 INFO] Step 6950/10000; acc:  93.18; ppl:  1.17; xent: 0.16; lr: 1.00000;   0/8936 tok/s;    576 sec\n",
            "[2019-06-30 09:15:53,217 INFO] Step 7000/10000; acc:  91.89; ppl:  1.22; xent: 0.20; lr: 1.00000;   0/8764 tok/s;    580 sec\n",
            "[2019-06-30 09:15:53,225 INFO] Loading dataset from models/trk.model-prepared_training_data.valid.0.pt, number of examples: 1229\n",
            "[2019-06-30 09:15:54,929 INFO] Validation perplexity: 11.2412\n",
            "[2019-06-30 09:15:54,929 INFO] Validation accuracy: 75.2123\n",
            "[2019-06-30 09:15:54,930 INFO] Saving checkpoint models/trk.model_step_7000.pt\n",
            "[2019-06-30 09:15:58,944 INFO] Step 7050/10000; acc:  92.47; ppl:  1.20; xent: 0.18; lr: 1.00000;   0/6165 tok/s;    585 sec\n",
            "[2019-06-30 09:16:03,159 INFO] Step 7100/10000; acc:  93.03; ppl:  1.18; xent: 0.16; lr: 1.00000;   0/8883 tok/s;    589 sec\n",
            "[2019-06-30 09:16:07,191 INFO] Step 7150/10000; acc:  92.38; ppl:  1.20; xent: 0.18; lr: 1.00000;   0/8896 tok/s;    593 sec\n",
            "[2019-06-30 09:16:11,006 INFO] Loading dataset from models/trk.model-prepared_training_data.train.0.pt, number of examples: 76665\n",
            "[2019-06-30 09:16:12,206 INFO] Step 7200/10000; acc:  92.97; ppl:  1.18; xent: 0.17; lr: 1.00000;   0/7299 tok/s;    598 sec\n",
            "[2019-06-30 09:16:16,181 INFO] Step 7250/10000; acc:  92.57; ppl:  1.19; xent: 0.17; lr: 1.00000;   0/8813 tok/s;    602 sec\n",
            "[2019-06-30 09:16:20,115 INFO] Step 7300/10000; acc:  91.92; ppl:  1.21; xent: 0.19; lr: 1.00000;   0/9010 tok/s;    606 sec\n",
            "[2019-06-30 09:16:24,219 INFO] Step 7350/10000; acc:  92.82; ppl:  1.18; xent: 0.17; lr: 1.00000;   0/9050 tok/s;    611 sec\n",
            "[2019-06-30 09:16:28,271 INFO] Step 7400/10000; acc:  92.56; ppl:  1.19; xent: 0.18; lr: 1.00000;   0/8786 tok/s;    615 sec\n",
            "[2019-06-30 09:16:32,257 INFO] Step 7450/10000; acc:  92.80; ppl:  1.18; xent: 0.17; lr: 1.00000;   0/8981 tok/s;    619 sec\n",
            "[2019-06-30 09:16:36,292 INFO] Step 7500/10000; acc:  91.98; ppl:  1.20; xent: 0.18; lr: 1.00000;   0/8940 tok/s;    623 sec\n",
            "[2019-06-30 09:16:40,162 INFO] Step 7550/10000; acc:  92.00; ppl:  1.21; xent: 0.19; lr: 1.00000;   0/8938 tok/s;    626 sec\n",
            "[2019-06-30 09:16:44,219 INFO] Step 7600/10000; acc:  93.17; ppl:  1.17; xent: 0.16; lr: 1.00000;   0/8932 tok/s;    631 sec\n",
            "[2019-06-30 09:16:48,190 INFO] Step 7650/10000; acc:  92.49; ppl:  1.19; xent: 0.18; lr: 1.00000;   0/8954 tok/s;    634 sec\n",
            "[2019-06-30 09:16:52,223 INFO] Step 7700/10000; acc:  92.61; ppl:  1.19; xent: 0.18; lr: 1.00000;   0/8934 tok/s;    639 sec\n",
            "[2019-06-30 09:16:56,433 INFO] Step 7750/10000; acc:  93.06; ppl:  1.18; xent: 0.16; lr: 1.00000;   0/8761 tok/s;    643 sec\n",
            "[2019-06-30 09:17:00,304 INFO] Step 7800/10000; acc:  91.79; ppl:  1.21; xent: 0.19; lr: 1.00000;   0/8838 tok/s;    647 sec\n",
            "[2019-06-30 09:17:04,330 INFO] Step 7850/10000; acc:  92.77; ppl:  1.19; xent: 0.17; lr: 1.00000;   0/8977 tok/s;    651 sec\n",
            "[2019-06-30 09:17:08,497 INFO] Step 7900/10000; acc:  93.34; ppl:  1.17; xent: 0.16; lr: 1.00000;   0/8782 tok/s;    655 sec\n",
            "[2019-06-30 09:17:12,343 INFO] Step 7950/10000; acc:  92.24; ppl:  1.20; xent: 0.18; lr: 1.00000;   0/9050 tok/s;    659 sec\n",
            "[2019-06-30 09:17:16,606 INFO] Step 8000/10000; acc:  92.87; ppl:  1.18; xent: 0.17; lr: 1.00000;   0/8701 tok/s;    663 sec\n",
            "[2019-06-30 09:17:16,613 INFO] Loading dataset from models/trk.model-prepared_training_data.valid.0.pt, number of examples: 1229\n",
            "[2019-06-30 09:17:18,271 INFO] Validation perplexity: 11.2716\n",
            "[2019-06-30 09:17:18,272 INFO] Validation accuracy: 74.57\n",
            "[2019-06-30 09:17:18,272 INFO] Saving checkpoint models/trk.model_step_8000.pt\n",
            "[2019-06-30 09:17:22,349 INFO] Step 8050/10000; acc:  92.55; ppl:  1.19; xent: 0.17; lr: 1.00000;   0/6418 tok/s;    669 sec\n",
            "[2019-06-30 09:17:26,297 INFO] Step 8100/10000; acc:  92.28; ppl:  1.20; xent: 0.19; lr: 1.00000;   0/8788 tok/s;    673 sec\n",
            "[2019-06-30 09:17:30,412 INFO] Step 8150/10000; acc:  93.39; ppl:  1.16; xent: 0.15; lr: 1.00000;   0/8876 tok/s;    677 sec\n",
            "[2019-06-30 09:17:34,374 INFO] Step 8200/10000; acc:  92.28; ppl:  1.21; xent: 0.19; lr: 1.00000;   0/8780 tok/s;    681 sec\n",
            "[2019-06-30 09:17:38,286 INFO] Step 8250/10000; acc:  92.68; ppl:  1.19; xent: 0.17; lr: 1.00000;   0/8956 tok/s;    685 sec\n",
            "[2019-06-30 09:17:42,460 INFO] Step 8300/10000; acc:  93.01; ppl:  1.18; xent: 0.16; lr: 1.00000;   0/8876 tok/s;    689 sec\n",
            "[2019-06-30 09:17:46,565 INFO] Step 8350/10000; acc:  92.82; ppl:  1.19; xent: 0.17; lr: 1.00000;   0/8873 tok/s;    693 sec\n",
            "[2019-06-30 09:17:50,165 INFO] Loading dataset from models/trk.model-prepared_training_data.train.0.pt, number of examples: 76665\n",
            "[2019-06-30 09:17:51,515 INFO] Step 8400/10000; acc:  93.23; ppl:  1.17; xent: 0.16; lr: 1.00000;   0/7356 tok/s;    698 sec\n",
            "[2019-06-30 09:17:55,503 INFO] Step 8450/10000; acc:  92.82; ppl:  1.18; xent: 0.17; lr: 1.00000;   0/8792 tok/s;    702 sec\n",
            "[2019-06-30 09:17:59,430 INFO] Step 8500/10000; acc:  92.06; ppl:  1.21; xent: 0.19; lr: 1.00000;   0/8961 tok/s;    706 sec\n",
            "[2019-06-30 09:18:03,596 INFO] Step 8550/10000; acc:  93.23; ppl:  1.17; xent: 0.16; lr: 1.00000;   0/9024 tok/s;    710 sec\n",
            "[2019-06-30 09:18:07,591 INFO] Step 8600/10000; acc:  92.60; ppl:  1.19; xent: 0.17; lr: 1.00000;   0/8817 tok/s;    714 sec\n",
            "[2019-06-30 09:18:11,616 INFO] Step 8650/10000; acc:  92.87; ppl:  1.18; xent: 0.16; lr: 1.00000;   0/8915 tok/s;    718 sec\n",
            "[2019-06-30 09:18:15,675 INFO] Step 8700/10000; acc:  92.31; ppl:  1.19; xent: 0.18; lr: 1.00000;   0/8944 tok/s;    722 sec\n",
            "[2019-06-30 09:18:19,502 INFO] Step 8750/10000; acc:  92.15; ppl:  1.20; xent: 0.19; lr: 1.00000;   0/8892 tok/s;    726 sec\n",
            "[2019-06-30 09:18:23,598 INFO] Step 8800/10000; acc:  93.41; ppl:  1.17; xent: 0.15; lr: 1.00000;   0/8905 tok/s;    730 sec\n",
            "[2019-06-30 09:18:27,549 INFO] Step 8850/10000; acc:  92.83; ppl:  1.18; xent: 0.17; lr: 1.00000;   0/8941 tok/s;    734 sec\n",
            "[2019-06-30 09:18:31,587 INFO] Step 8900/10000; acc:  92.85; ppl:  1.18; xent: 0.17; lr: 1.00000;   0/8935 tok/s;    738 sec\n",
            "[2019-06-30 09:18:35,801 INFO] Step 8950/10000; acc:  93.41; ppl:  1.16; xent: 0.15; lr: 1.00000;   0/8789 tok/s;    742 sec\n",
            "[2019-06-30 09:18:39,652 INFO] Step 9000/10000; acc:  92.08; ppl:  1.21; xent: 0.19; lr: 1.00000;   0/8827 tok/s;    746 sec\n",
            "[2019-06-30 09:18:39,660 INFO] Loading dataset from models/trk.model-prepared_training_data.valid.0.pt, number of examples: 1229\n",
            "[2019-06-30 09:18:41,297 INFO] Validation perplexity: 12.1078\n",
            "[2019-06-30 09:18:41,297 INFO] Validation accuracy: 75.0196\n",
            "[2019-06-30 09:18:41,298 INFO] Saving checkpoint models/trk.model_step_9000.pt\n",
            "[2019-06-30 09:18:45,367 INFO] Step 9050/10000; acc:  93.01; ppl:  1.18; xent: 0.16; lr: 1.00000;   0/6358 tok/s;    752 sec\n",
            "[2019-06-30 09:18:49,558 INFO] Step 9100/10000; acc:  93.44; ppl:  1.16; xent: 0.15; lr: 1.00000;   0/8791 tok/s;    756 sec\n",
            "[2019-06-30 09:18:53,387 INFO] Step 9150/10000; acc:  92.56; ppl:  1.19; xent: 0.18; lr: 1.00000;   0/9087 tok/s;    760 sec\n",
            "[2019-06-30 09:18:57,628 INFO] Step 9200/10000; acc:  93.00; ppl:  1.18; xent: 0.16; lr: 1.00000;   0/8706 tok/s;    764 sec\n",
            "[2019-06-30 09:19:01,667 INFO] Step 9250/10000; acc:  92.63; ppl:  1.18; xent: 0.17; lr: 1.00000;   0/9054 tok/s;    768 sec\n",
            "[2019-06-30 09:19:05,606 INFO] Step 9300/10000; acc:  92.75; ppl:  1.18; xent: 0.17; lr: 1.00000;   0/8851 tok/s;    772 sec\n",
            "[2019-06-30 09:19:09,706 INFO] Step 9350/10000; acc:  93.52; ppl:  1.16; xent: 0.15; lr: 1.00000;   0/8974 tok/s;    776 sec\n",
            "[2019-06-30 09:19:13,680 INFO] Step 9400/10000; acc:  92.28; ppl:  1.20; xent: 0.18; lr: 1.00000;   0/8769 tok/s;    780 sec\n",
            "[2019-06-30 09:19:17,581 INFO] Step 9450/10000; acc:  92.82; ppl:  1.18; xent: 0.17; lr: 1.00000;   0/8913 tok/s;    784 sec\n",
            "[2019-06-30 09:19:21,730 INFO] Step 9500/10000; acc:  93.36; ppl:  1.16; xent: 0.15; lr: 1.00000;   0/8903 tok/s;    788 sec\n",
            "[2019-06-30 09:19:25,909 INFO] Step 9550/10000; acc:  93.35; ppl:  1.17; xent: 0.15; lr: 1.00000;   0/8828 tok/s;    792 sec\n",
            "[2019-06-30 09:19:29,329 INFO] Loading dataset from models/trk.model-prepared_training_data.train.0.pt, number of examples: 76665\n",
            "[2019-06-30 09:19:30,824 INFO] Step 9600/10000; acc:  93.11; ppl:  1.17; xent: 0.16; lr: 1.00000;   0/7320 tok/s;    797 sec\n",
            "[2019-06-30 09:19:34,821 INFO] Step 9650/10000; acc:  93.02; ppl:  1.17; xent: 0.16; lr: 1.00000;   0/8822 tok/s;    801 sec\n",
            "[2019-06-30 09:19:38,722 INFO] Step 9700/10000; acc:  92.21; ppl:  1.20; xent: 0.18; lr: 1.00000;   0/9014 tok/s;    805 sec\n",
            "[2019-06-30 09:19:42,873 INFO] Step 9750/10000; acc:  93.08; ppl:  1.17; xent: 0.16; lr: 1.00000;   0/9010 tok/s;    809 sec\n",
            "[2019-06-30 09:19:46,881 INFO] Step 9800/10000; acc:  92.93; ppl:  1.18; xent: 0.16; lr: 1.00000;   0/8836 tok/s;    813 sec\n",
            "[2019-06-30 09:19:50,880 INFO] Step 9850/10000; acc:  93.05; ppl:  1.17; xent: 0.16; lr: 1.00000;   0/8941 tok/s;    817 sec\n",
            "[2019-06-30 09:19:54,953 INFO] Step 9900/10000; acc:  92.58; ppl:  1.18; xent: 0.17; lr: 1.00000;   0/8930 tok/s;    821 sec\n",
            "[2019-06-30 09:19:58,801 INFO] Step 9950/10000; acc:  92.38; ppl:  1.19; xent: 0.18; lr: 1.00000;   0/8844 tok/s;    825 sec\n",
            "[2019-06-30 09:20:02,877 INFO] Step 10000/10000; acc:  93.57; ppl:  1.16; xent: 0.15; lr: 1.00000;   0/8947 tok/s;    829 sec\n",
            "[2019-06-30 09:20:02,884 INFO] Loading dataset from models/trk.model-prepared_training_data.valid.0.pt, number of examples: 1229\n",
            "[2019-06-30 09:20:04,529 INFO] Validation perplexity: 12.1119\n",
            "[2019-06-30 09:20:04,529 INFO] Validation accuracy: 75.1267\n",
            "[2019-06-30 09:20:04,529 INFO] Saving checkpoint models/trk.model_step_10000.pt\n",
            "*===QUALITY ON VAL DATA===*\n",
            "Recall for analysis: 52.40\n",
            "Precision for analysis: 36.82\n",
            "F1-score for analysis: 43.25\n",
            "\n",
            "Recall for lemma: 68.23\n",
            "Precision for lemma: 52.00\n",
            "F1-score for lemma: 59.02\n",
            "\n",
            "Recall for tag: 65.15\n",
            "Precision for tag: 50.47\n",
            "F1-score for tag: 56.88\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCLuVn1Yysdq",
        "colab_type": "code",
        "outputId": "2cb71c30-59a0-4a44-8f12-c6106ae5bf3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "tmp = pd.read_csv('results/tmp.txt', sep='\\t', names=['iso', 'wf', 'lemma', 'pos', 'tags'])\n",
        "tmp.head()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>iso</th>\n",
              "      <th>wf</th>\n",
              "      <th>lemma</th>\n",
              "      <th>pos</th>\n",
              "      <th>tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>krc</td>\n",
              "      <td>эм</td>\n",
              "      <td>эм</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>Case=Nom</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>krc</td>\n",
              "      <td>эм</td>\n",
              "      <td>эм</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>Case=Nom</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>krc</td>\n",
              "      <td>эм</td>\n",
              "      <td>эм</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>Case=Nom</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>krc</td>\n",
              "      <td>болгъанды</td>\n",
              "      <td>болгъан</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>Case=Acc</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>krc</td>\n",
              "      <td>кёре</td>\n",
              "      <td>кёре</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>Case=Nom</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   iso         wf    lemma   pos      tags\n",
              "0  krc         эм       эм  NOUN  Case=Nom\n",
              "1  krc         эм       эм  NOUN  Case=Nom\n",
              "2  krc         эм       эм  NOUN  Case=Nom\n",
              "3  krc  болгъанды  болгъан  NOUN  Case=Acc\n",
              "4  krc       кёре     кёре  NOUN  Case=Nom"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKT6JM9Fzp3O",
        "colab_type": "code",
        "outputId": "34c0b646-d1d3-4976-96b7-86023712bc57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "calm = calm[calm.tags != 'bad_verb']\n",
        "calm.tags.value_counts()[:5]"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Case=Nom                                          3433\n",
              "Case=Acc                                          1603\n",
              "Case=Nom|Number[psor]=Sing,Plur|Person[psor]=3    1170\n",
              "_                                                  957\n",
              "Case=Loc                                           866\n",
              "Name: tags, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFx5A5P-27Zb",
        "colab_type": "code",
        "outputId": "b8d7c992-f342-4c20-d7b1-2c6c98ff5a01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "my_model_predictions = pd.concat([calm, tmp])\n",
        "my_model_predictions.tags.value_counts()[:5]"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Case=Nom                                          4301\n",
              "Case=Acc                                          1942\n",
              "Case=Nom|Number[psor]=Sing,Plur|Person[psor]=3    1389\n",
              "_                                                 1274\n",
              "Case=Loc                                          1230\n",
              "Name: tags, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLxvQ4TW5ZlF",
        "colab_type": "code",
        "outputId": "c77b3fa5-d8fe-4361-fcbb-0849ec9f4288",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "my_model_predictions.head()"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>iso</th>\n",
              "      <th>wf</th>\n",
              "      <th>lemma</th>\n",
              "      <th>pos</th>\n",
              "      <th>tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>krc</td>\n",
              "      <td>джыл</td>\n",
              "      <td>джыл</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>Case=Nom</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>krc</td>\n",
              "      <td>джыл</td>\n",
              "      <td>джыл</td>\n",
              "      <td>ADJ</td>\n",
              "      <td>_</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>krc</td>\n",
              "      <td>эм</td>\n",
              "      <td>эм</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>Case=Nom</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>krc</td>\n",
              "      <td>джылны</td>\n",
              "      <td>джыл</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>Case=Acc</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>krc</td>\n",
              "      <td>дери</td>\n",
              "      <td>дер</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>Case=Nom|Number[psor]=Sing,Plur|Person[psor]=3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   iso      wf lemma   pos                                            tags\n",
              "0  krc    джыл  джыл  NOUN                                        Case=Nom\n",
              "1  krc    джыл  джыл   ADJ                                               _\n",
              "2  krc      эм    эм  NOUN                                        Case=Nom\n",
              "4  krc  джылны  джыл  NOUN                                        Case=Acc\n",
              "6  krc    дери   дер  NOUN  Case=Nom|Number[psor]=Sing,Plur|Person[psor]=3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKA0sgiO6V_1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_model_predictions.to_csv('spectie.txt', sep='\\t', header=False, index=False, encoding='utf-8')\n",
        "files.download('spectie.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}