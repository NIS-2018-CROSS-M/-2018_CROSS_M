{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "get fasttext.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "jpEItKJgELqM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vardial2fasttext = {\n",
        "    \"ast\":\"ast\",\n",
        "    \"cat\":\"ca\",\n",
        "    \"fra\":\"fr\",\n",
        "    \"ita\":\"it\",\n",
        "    \"por\":\"pt\",\n",
        "    \"spa\":\"es\",\n",
        "    \"bak\":\"ba\",\n",
        "    \"crh\":\"crh\",\n",
        "    \"kaz\":\"kk\",\n",
        "    \"kir\":\"ky\",\n",
        "    \"tat\":\"tt\",\n",
        "    \"tur\":\"tr\"\n",
        "}\n",
        "fasttext2vardial = {ft_code:vd_code for vd_code,ft_code in vardial2fasttext.items()}\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OwcXpSuTFbTM",
        "colab_type": "code",
        "outputId": "300f85e6-d27c-4b70-80f6-055aaa52df6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        }
      },
      "cell_type": "code",
      "source": [
        "langs_to_process = {\"roa\" : (\"ast\", \"cat\", \"fra\", \"ita\", \"por\", \"spa\"), \"trk\":()}\n",
        "for group_name, group_langs in langs_to_process.items():\n",
        "\n",
        "    !wget -O train-{group_name}-uncovered -q https://raw.githubusercontent.com/ftyers/vardial-shared-task/master/train/{group_name}-uncovered\n",
        "    !wget -O dev-{group_name}-uncovered -q https://raw.githubusercontent.com/ftyers/vardial-shared-task/master/dev/{group_name}-uncovered\n",
        "    !wget -O dev-{group_name}-covered -q https://raw.githubusercontent.com/ftyers/vardial-shared-task/master/dev/{group_name}-covered\n",
        "\n",
        "    !cat  train-{group_name}-uncovered >> {group_name}-alldata\n",
        "    !cat  dev-{group_name}-uncovered >> {group_name}-alldata\n",
        "    !cat  dev-{group_name}-covered >> {group_name}-alldata\n",
        "    \n",
        "    for lang_to_process in group_langs:\n",
        "        lang_ft_code = vardial2fasttext[lang_to_process]\n",
        "        !wget -q https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.{lang_ft_code}.vec\n",
        "        \n",
        "        \n",
        "        alphabet = !cut -f2 {group_name}-alldata | grep -o . | sort -u\n",
        "        for idx in range(len(alphabet)):\n",
        "            elem = alphabet[idx]\n",
        "            if elem in ['-','^',']']:\n",
        "                alphabet[idx] = '\\\\'+elem\n",
        "        alphabet = \"\".join(c for c in alphabet)\n",
        "        \n",
        "        vec_words_fn = f\"{lang_ft_code}_vec_short\"\n",
        "        tracked_words_fn = f\"{lang_to_process}_tracked\"\n",
        "        \n",
        "        get_ipython().system(f\"grep -P \\\"^{lang_to_process}\\\" {group_name}-alldata | cut -f2 | sort -u > {tracked_words_fn}\")\n",
        "        \n",
        "        cmd_to_exec = f\"grep -P \\\"^[{alphabet}]+\\s\\\" wiki.{lang_ft_code}.vec | \" + \"grep -v -P \\\"^\\w*\\d\\w*\\\" | cut -d$\\' \\' -f1 > \" + f\"{vec_words_fn}\"\n",
        "        get_ipython().system(cmd_to_exec)\n",
        "        \n",
        "        words_having_vectors_fn = f\"{lang_to_process}_vectorized_words\"\n",
        "        words_non_having_vectors_fn = f\"{lang_to_process}_nonvectorized_words\"\n",
        "        \n",
        "        with open(tracked_words_fn, 'r', encoding='utf-8') as f_tracked,\\\n",
        "            open(vec_words_fn, 'r', encoding='utf-8') as f_vectors,\\\n",
        "            open(words_having_vectors_fn, 'w', encoding='utf-8') as f_vectorized,\\\n",
        "            open(words_non_having_vectors_fn, 'w', encoding='utf-8') as f_non_vectorized:\n",
        "            \n",
        "            track_words = set([line.strip() for line in f_tracked.readlines()])\n",
        "            for line in f_vectors:\n",
        "                line = line.strip()\n",
        "                if line in track_words:\n",
        "                    print(line, file=f_vectorized)\n",
        "                else:\n",
        "                    print(line, file=f_non_vectorized)\n",
        "                    \n",
        "        num_words_in_track = len(track_words)\n",
        "        num_words_vectorized = !wc -l {words_having_vectors_fn} | cut -d$' ' -f1\n",
        "        \n",
        "        num_words_vectorized = int(num_words_vectorized[0])\n",
        "        num_words_non_vectorized = num_words_in_track - num_words_vectorized\n",
        "        \n",
        "        print(\", \".join([f\"lang: {lang_to_process:>3}\",\n",
        "              f\"words in track: {num_words_in_track:>6}\",\n",
        "              f\"vectorized words num: {num_words_vectorized:>6}\",\n",
        "              f\"non vectorized words num: {num_words_non_vectorized:>6}\"]))\n",
        "        \n",
        "        tracked_vectors_fn = f\"{lang_to_process}_tracked.vec\"\n",
        "        with open(f\"wiki.{lang_ft_code}.vec\", 'r', encoding=\"utf-8\") as vectors_f, \\\n",
        "            open(tracked_vectors_fn, 'w', encoding=\"utf-8\") as tracked_vectors_f:\n",
        "                for line in vectors_f:\n",
        "                    if line.strip().split()[0] in track_words:\n",
        "                        print(line.strip(), file=tracked_vectors_f)\n",
        "                        \n",
        "            "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lang: ast, words in track:   1000, vectorized words num:   1000, non vectorized words num:      0\n",
            "lang: cat, words in track:  10000, vectorized words num:   9973, non vectorized words num:     27\n",
            "lang: fra, words in track:   9986, vectorized words num:   9912, non vectorized words num:     74\n",
            "lang: ita, words in track:   9998, vectorized words num:   9988, non vectorized words num:     10\n",
            "lang: por, words in track:   9999, vectorized words num:   9974, non vectorized words num:     25\n",
            "lang: spa, words in track:   9999, vectorized words num:   9999, non vectorized words num:      0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RTjIsn_9Hg6s",
        "colab_type": "code",
        "outputId": "a73e09ff-4fb7-4210-8252-6b96a3e6c165",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ast_nonvectorized_words  fra_nonvectorized_words  roa-alldata\n",
            "ast_tracked\t\t fra_tracked\t\t  sample_data\n",
            "ast_tracked.vec\t\t fra_tracked.vec\t  spa_nonvectorized_words\n",
            "ast_vec_short\t\t fra_vectorized_words\t  spa_tracked\n",
            "ast_vectorized_words\t fr_vec_short\t\t  spa_tracked.vec\n",
            "cat_nonvectorized_words  ita_nonvectorized_words  spa_vectorized_words\n",
            "cat_tracked\t\t ita_tracked\t\t  train-roa-uncovered\n",
            "cat_tracked.vec\t\t ita_tracked.vec\t  train-trk-uncovered\n",
            "cat_vectorized_words\t ita_vectorized_words\t  trk-alldata\n",
            "ca_vec_short\t\t it_vec_short\t\t  wiki.ast.vec\n",
            "dev-roa-covered\t\t por_nonvectorized_words  wiki.ca.vec\n",
            "dev-roa-uncovered\t por_tracked\t\t  wiki.es.vec\n",
            "dev-trk-covered\t\t por_tracked.vec\t  wiki.fr.vec\n",
            "dev-trk-uncovered\t por_vectorized_words\t  wiki.it.vec\n",
            "es_vec_short\t\t pt_vec_short\t\t  wiki.pt.vec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dYNKdVpzKjzz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}