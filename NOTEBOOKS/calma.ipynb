{"cells":[{"cell_type":"markdown","source":["  ## set up telegram notifications\n","\n","  не очень понятно, нужно ли это. если нужно -- напишите @oserikov в телеграме, я расскажу, что сделать, чтобы присылались сообщения с качеством модели когда она отработает."],"metadata":{}},{"source":["telegram_notifications_enabled=False\n","EXP_DESCRIPTION = \"BASELINE\"\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["if telegram_notifications_enabled:\n","    bot_token = input(\"введите telegram bot token: \")\n","    chat_id = \"292749902\" # for @oserikov\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":["  # prepare"],"metadata":{}},{"source":["import sys\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":["  #### install prereqs"],"metadata":{}},{"source":["\n","# clone openmt-py used in calma and move it into the proper folder\n","get_ipython().system('git clone https://github.com/OpenNMT/OpenNMT-py.git')\n","get_ipython().magic('cd OpenNMT-py')\n","get_ipython().system('git checkout -b stable-version d57fa68e6b0c2041642af40f76e1d5903c80a9b8')\n","get_ipython().magic('cd ..')\n","get_ipython().system('mv OpenNMT-py ~')\n","get_ipython().system('wget -q https://raw.githubusercontent.com/NIS-2018-CROSS-M/calma/tmp-utils/utils/onmt-decoder.py -O ~/OpenNMT-py/onmt/decoders/decoder.py')\n","get_ipython().system('wget -q https://raw.githubusercontent.com/NIS-2018-CROSS-M/calma/tmp-utils/utils/onmt-opts.py -O ~/OpenNMT-py/onmt/opts.py')\n","\n","# clone and run a tool installing pytorch 0.4.1 with cuda 9.2 into colab (maybe works on any ubuntu 16)\n","get_ipython().system('git clone https://gist.github.com/f7b7c7758a46da49f84bc68b47997d69.git colab_cuda_upgrader')\n","get_ipython().system('bash colab_cuda_upgrader/pytorch041_cuda92_colab.sh')\n","\n","# install dependencies used in calma project\n","get_ipython().system('{sys.executable} -m pip install configargparse')\n","\n","# install the proper version of torchtext\n","get_ipython().system('git clone https://github.com/pytorch/text.git')\n","get_ipython().magic('cd text')\n","get_ipython().system('{sys.executable} -m pip install .')\n","get_ipython().magic('cd ..')\n","\n","# receive the calma\n","get_ipython().system('git clone https://github.com/ftyers/calma.git')\n","get_ipython().magic('cd calma')\n","get_ipython().system('git checkout -b latest-known-version d4ce3758d06538933855f734a44630efc8e2b6b2')\n","get_ipython().magic('cd sharedtaskdata')\n","get_ipython().system('rm onmt-data/*')\n","get_ipython().system('rm results/*')\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":["  #### imports"],"metadata":{}},{"source":["from collections import defaultdict as dd\n","from random import shuffle\n","import re\n","import urllib\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":["  ## helping functions definitions"],"metadata":{}},{"source":["def score_predictions(res_file_fn, gold_file_fn, output_fn, dataEvaluator):   \n","    \n","    def readdata(fn):\n","        data = {otype:dd(set) for otype in dataEvaluator.otypes}\n","        for line in open(fn):\n","            line = line.strip('\\n')\n","            if line:\n","                data = dataEvaluator.update_data(data, line)\n","        return data\n","    \n","    sysdata = readdata(res_file_fn)\n","    golddata = readdata(gold_file_fn)\n","    \n","    output_f = open(output_fn, 'a+', encoding='utf-8')\n","    for otype in dataEvaluator.otypes:\n","        tp = 0\n","        fp = 0\n","        fn = 0\n","        for wf in sysdata[otype]:\n","            tp += len(sysdata[otype][wf] & golddata[otype][wf])\n","            fp += len(sysdata[otype][wf] - golddata[otype][wf])\n","            fn += len(golddata[otype][wf] - sysdata[otype][wf])\n","        recall = tp/(tp+fn)\n","        precision = tp/(tp+fp)\n","        fscore = 2 * recall * precision / (recall + precision)\n","        print(\"Recall for %s: %.2f\" % (otype,recall*100), file = output_f)\n","        print(\"Precision for %s: %.2f\" % (otype,precision*100), file = output_f)\n","        print(\"F1-score for %s: %.2f\" % (otype,fscore*100), file = output_f)\n","        print(\"\", file = output_f)\n","    \n","    output_f.close()\n","\n","def modify_nbest(nbest_src_filename, nbest_tgt_filename, nbestModifyer):\n","    with open(nbest_src_filename, 'r', encoding='utf-8') as src_f,         open(nbest_tgt_filename, 'w', encoding='utf-8') as tgt_f:\n","\n","        for line in src_f.readlines():\n","            line = line.rstrip('\\n').rstrip('\\r')\n","            if line.startswith(\"SENT \"):\n","                line = nbestModifyer.sent_to_baseline_compatible(line)\n","            elif re.match(\"^\\[[\\-\\+]?\\d+\\.\\d+\\]\\s\\[\", line):\n","                line = nbestModifyer.hyp_to_baseline_compatible(line)\n","\n","            print(line, file=tgt_f)\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["def initialize_data(train_src, train_tgt, valid_src, valid_tgt, prepared_training_data_prefix):\n","    prepr_params = f\"-train_src {train_src} -train_tgt {train_tgt} -valid_src {valid_src} -valid_tgt {valid_tgt} -save_data {prepared_training_data_prefix}\" \n","    get_ipython().system('{sys.executable} ~/OpenNMT-py/preprocess.py {prepr_params}')\n","\n","\n","def train_ml(train_params):\n","    train_params = \" \".join(train_params)\n","    get_ipython().system('{sys.executable} ~/OpenNMT-py/train.py {train_params}')\n","\n","\n","def generate_predictions(generation_params, output_filename):\n","    generation_params = \" \".join(generation_params)\n","    get_ipython().system('{sys.executable} ~/OpenNMT-py/translate.py {generation_params} > {output_filename}')\n","\n","\n","def choose_best_predictions(nbest_filename, covered_filename, output_filename):\n","    get_ipython().system('cat {nbest_filename} | grep -v -P \"^\\\\s+\" | grep -v -P \"^\\\\+\" | {sys.executable} scripts/get-analyses.py 0.8 3 {covered_filename} > {output_filename}')\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":["  #### def data_generation"],"metadata":{}},{"source":["# the method called for each non-processed training data row\n","def get_data_entry(language, wordform, lemma, pos_tag, morphological_analysis):\n","    lemma = ' '.join(lemma)\n","    wordform = ' '.join(wordform)\n","    morphological_analysis = morphological_analysis.split('|')\n","    return wordform, '%s %s' % (lemma, ' '.join(['+%s' % x for x in [pos_tag] + morphological_analysis  + [\"Language=%s\" % language]]))\n","\n","\n","def generate_onmt_data(fn, res_src_fn, res_tgt_fn, DataModifyerClass):\n","    \n","    modify_src_line = DataModifyerClass.modify_src_line\n","    modify_tgt_line = DataModifyerClass.modify_tgt_line\n","    restore_orig_src_line = DataModifyerClass.restore_src_line\n","    restore_orig_tgt_line = DataModifyerClass.restore_tgt_line\n","    \n","    analyses = dd(set)\n","\n","    for line in open(fn, encoding='utf-8'):\n","        line = line.rstrip('\\n').rstrip('\\r')\n","        lang, wf, lemma, pos, msd = line.split('\\t')\n","        wf, a = get_data_entry(lang, wf, lemma, pos, msd)\n","        analyses[wf].add(a)\n","    \n","    tmp_src_fn = res_src_fn + \"-default\"\n","    tmp_tgt_fn = res_tgt_fn + \"-default\"\n","    \n","    tmp_src = open(tmp_src_fn, 'w')\n","    tmp_tgt = open(tmp_tgt_fn, 'w')\n","    res_src = open(res_src_fn, 'w')\n","    res_tgt = open(res_tgt_fn, 'w')\n","    \n","    analyses = list(analyses.items())\n","    shuffle(analyses)\n","\n","    for wf, analysis in analyses:\n","        for a in analysis:\n","            print(wf, file = tmp_src)\n","            print(a, file = tmp_tgt)\n","            print(modify_src_line(wf), file = res_src)\n","            print(modify_tgt_line(a), file = res_tgt)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":["  ### def ml()"],"metadata":{}},{"source":["class MLUtil:\n","    def __init__(self, prediction_params, dataModifyer, nbestModifyer):\n","        self.dataModifyer = dataModifyer\n","        self.nbestModifyer = nbestModifyer\n","        self.prediction_params = prediction_params\n","\n","    \n","    def generate_data(self, orig_data_fn, res_src_fn, res_tgt_fn):\n","        return generate_onmt_data(orig_data_fn, res_src_fn, res_tgt_fn, self.dataModifyer)\n","\n","    def train(self, train_res_src_fn, train_res_tgt_fn, val_res_src_fn, val_res_tgt_fn, save_model_fn, train_params):\n","        data_fn = save_model_fn + \"-prepared_training_data\" #f\"onmt-data/{lang}-track{track}\"    \n","        initialize_data(train_res_src_fn, train_res_tgt_fn, val_res_src_fn, val_res_tgt_fn, data_fn)\n","\n","        train_params.extend([f\"-data {data_fn}\", f\"-save_model {save_model_fn}\"])\n","        train_ml(train_params)\n","        get_ipython().system('mv {save_model_fn}_step_{train_steps}.pt {save_model_fn}')\n","\n","    \n","    def predict(self, model_filename, input_data_filename, covered_filename, chosen_output_filename):\n","        output_data_filename = f\"{input_data_filename}.out\"\n","        nbest_output_filename = f\"{input_data_filename}.nbest.out\"\n","        self.prediction_params.extend([\n","            f\"-model {model_filename}\",\n","            f\"-src {input_data_filename}\",\n","            f\"-output {output_data_filename}\"\n","        ])\n","        generate_predictions(self.prediction_params, nbest_output_filename)\n","        nbest_output_modified_filename = nbest_output_filename+\"-modified\"\n","        modify_nbest(nbest_output_filename, nbest_output_modified_filename, self.nbestModifyer)\n","        choose_best_predictions(nbest_output_modified_filename, covered_filename, chosen_output_filename)\n","        \n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":["  # ML"],"metadata":{}},{"cell_type":"markdown","source":["  ## set ml params"],"metadata":{}},{"source":["langs=['crh']\n","tracks=['2']\n","data_classes = ['test', 'dev']\n","\n","train_steps=1000\n","valid_steps=100\n","save_checkpoint_steps = valid_steps\n","\n","train_params = [\n","    f\"-train_steps {train_steps}\",\n","    f\"-valid_steps {valid_steps}\",\n","    f\"-save_checkpoint_steps {save_checkpoint_steps}\",\n","    f\"-world_size 1\",\n","    f\"-gpu_ranks 0 1\",\n","    f\"-encoder_type brnn\"\n","]\n","\n","pred_params = [\n","    f\"-replace_unk\",\n","    f\"-verbose\",\n","    f\"-n_best 8\",\n","    f\"-beam 8\"\n","]\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":["  ## Baseline approach"],"metadata":{}},{"cell_type":"markdown","source":["  #### data description"],"metadata":{}},{"cell_type":"markdown","source":["  **source**\n","  ```\n","  wf1 wf2 ... wfN\n","  ```\n","\n","  **target**\n","  ```\n","  l1 l2 ... lN +POS +Tag1=Value1 ... +TagN=ValueN +Language=langCode\n","  ```\n","\n","  **uncovered**\n","  (tab separated)\n","  ```\n","  langCode\twordForm\tlemma\tPOS\tTag1=Value1|...|TagN=ValueN\n","  ```\n","\n","  **prediction** raw\n","  ```\n","  SENT 1: ['wf1', 'wf2', ..., 'wfN']\n","  ...\n","  [-9.2825] ['c', 'o', 'n', 'v', 'i', 'd', 'u', '+NOUN', '+Gender=Masc', '+Number=Plur', '+Language=ast']\n","  ```\n","  **prediction** passed to `eval()`\n","  ```\n","  ['l1', 'l2', ..., 'lN', '+POS', '+Tag1=Value1', ..., '+TagN=ValueN', '+Language=langCode']\n","  ```\n","\n","  prediction then is converted to follow the uncovered file pattern\n","  ```\n","  langCode\twordForm\tlemma\tPOS\tTag1=Value1|...|TagN=ValueN\n","  ```\n","\n",""],"metadata":{}},{"cell_type":"markdown","source":["  #### embeddings\n","  * character-level input embeddings\n","  * character-level output embeddings\n","  * learned\n","  * initialized with random"],"metadata":{}},{"cell_type":"markdown","source":["  #### data modification"],"metadata":{}},{"source":["class TrainDataModifyer:\n","    def modify_src_line(line):\n","        return line\n","\n","\n","    def restore_src_line(line):\n","        return line\n","\n","\n","    def modify_tgt_line(line):\n","        return line\n","\n","\n","    def restore_tgt_line(line):\n","        return line\n","\n","\n","class NBestDataModifyer:\n","    def sent_to_baseline_compatible(line):\n","        return line\n","                       \n","    def hyp_to_baseline_compatible(line):\n","        return line\n","\n","    \n","class DataEvaluator:\n","    otypes =  [\"analysis\",\"lemma\",\"tag\"]\n","    \n","    def update_data(data, line):\n","        lan, wf, lemma, pos, msd = line.split('\\t')\n","        \n","        data[\"analysis\"][wf].add((lemma,pos,msd))\n","        data[\"lemma\"][wf].add(lemma)\n","        data[\"tag\"][wf].add((pos,msd))\n","        \n","        return data\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":["  #### ml"],"metadata":{}},{"source":["class CognatesTool:\n","    words_info = {}\n","\n","    def __init__(self, ud_data_filenames):\n","        for fn in ud_data_filenames:\n","            with open(fn, 'r', encoding=\"utf-8\") as f:\n","                for line in f:\n","                    lang, wf, lemma, pos, morph_a = line.rstrip().split('\\t')\n","                    if wf not in self.words_info.keys():\n","                        self.words_info[wf] = []\n","                    \n","                    self.words_info[wf].append(\n","                        {\n","                            \"lang\": lang,\n","                            \"wf\": wf,\n","                            \"lemma\": lemma,\n","                            \"pos\": pos,\n","                            \"morph_a\": morph_a\n","                        }\n","                    )\n","        print(str(len(self.words_info.keys())) + \" words have cognates\")\n","\n","    def get_words(self):\n","        return self.words_info.keys()\n","\n","    def word_has_cognates(self, word):\n","        return word in self.get_words()\n","\n","    def has_cognates(self, line, onmt_style=False):\n","        if onmt_style:\n","            res = self.word_has_cognates(''.join(line.rstrip().split()))\n","        else:\n","            # ud style lang \\t wordform \\t lemma \\t pos \\t analyses \\t i want to sleeeeep\n","            res = self.word_has_cognates(line.rstrip().split('\\t')[1])\n","        return res\n","\n","    def predict(self, src_filename, output_filename):\n","        with open(src_filename, 'r', encoding=\"utf-8\") as f_src,             open(output_filename, 'w', encoding=\"utf-8\") as f_tgt:\n","            \n","            for line in f_src:\n","                lang, wf, lemma, pos, morph_a = line.rstrip().split('\\t')\n","                \n","                if wf not in self.get_words():\n","                    continue                \n","                \n","                for analysis_set in self.words_info[wf]:\n","                    print('\\t'.join([analysis_set[\"lang\"],\n","                                     analysis_set[\"wf\"],\n","                                     analysis_set[\"lemma\"],\n","                                     analysis_set[\"pos\"],\n","                                     analysis_set[\"morph_a\"]]),\n","                          file=f_tgt)\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","def ml(langs, tracks, train_params, prediction_params, dataModifyer, nbestModifyer, dataEvaluator):\n","    mlUtil = MLUtil(prediction_params, dataModifyer, nbestModifyer)\n","    for lang in langs:\n","        for track in tracks:\n","            \n","            # filenames, many of them\n","            train_covered_filename = f\"train/{lang}-track{track}-covered\"\n","            train_uncovered_filename = f\"train/{lang}-track{track}-uncovered\"\n","            train_res_src_filename = f\"onmt-data/{lang}-track{track}-src-train.txt\"\n","            train_res_tgt_filename = f\"onmt-data/{lang}-track{track}-tgt-train.txt\"\n","            \n","            test_covered_filename = f\"test/{lang}-covered\"\n","            test_uncovered_filename = f\"test/{lang}-uncovered\"\n","            test_res_src_filename = f\"onmt-data/{lang}-track{track}-src-test.txt\"\n","            test_res_tgt_filename = f\"onmt-data/{lang}-track{track}-tgt-test.txt\"\n","            test_pred_output_filename = f\"results/{lang}-track{track}-test-covered.sys\" # output :)\n","            \n","            val_covered_filename = f\"dev/{lang}-covered\"\n","            val_uncovered_filename = f\"dev/{lang}-uncovered\"\n","            val_res_src_filename = f\"onmt-data/{lang}-track{track}-src-dev.txt\"\n","            val_res_tgt_filename = f\"onmt-data/{lang}-track{track}-tgt-dev.txt\"\n","            val_pred_output_filename = f\"results/{lang}-track{track}-dev-covered.sys\" # output :)\n","            \n","            \n","            model_filename = f\"models/{lang}-track{track}.model\"\n","\n","            score_log_filename = f\"{lang}-{track}-score.log\"\n","            get_ipython().system(f'touch {score_log_filename}')\n","            \n","            \n","            # ml| data preprocessing\n","            mlUtil.generate_data(train_uncovered_filename, train_res_src_filename, train_res_tgt_filename)        \n","            mlUtil.generate_data(val_uncovered_filename, val_res_src_filename, val_res_tgt_filename)\n","            mlUtil.generate_data(test_covered_filename, test_res_src_filename, test_res_tgt_filename)\n","            \n","            # ml| training\n","            mlUtil.train(train_res_src_filename, train_res_tgt_filename, val_res_src_filename, val_res_tgt_filename, model_filename, train_params)\n","            \n","            # ml| predict and eval for test\n","            mlUtil.predict(model_filename, test_res_src_filename, test_covered_filename, test_pred_output_filename)\n","            get_ipython().system(f'echo \"*===QUALITY ON TEST DATA===*\" >> {score_log_filename}')\n","            score_predictions(test_pred_output_filename, test_uncovered_filename, score_log_filename, dataEvaluator)\n","\n","            \n","            # ml| predict and eval for val\n","            \n","            # cognates heuristic: use ml for val data non having cognates, copy analysis of cognates\n","            cognates_tool = CognatesTool([train_uncovered_filename])\n","            # split the val data onto cognates-having(below) and cognates non-having\n","            val_res_src_for_ml_filename = val_res_src_filename+\"for_ml\"\n","            val_res_src_for_cognates_filename = val_res_src_filename+\"for_cognates\"\n","            with open(val_res_src_filename, 'r', encoding=\"utf-8\") as f,                 open(val_res_src_for_ml_filename, 'w', encoding=\"utf-8\") as f_ml,                 open(val_res_src_for_cognates_filename, 'w', encoding=\"utf-8\") as f_cog:\n","                for line in f:\n","                    print(line.rstrip(), file=f_cog if cognates_tool.has_cognates(line, onmt_style=True) else f_ml)\n","            # split the val data onto cognates-having and cognates non-having (below)\n","            val_covered_for_ml_filename = val_covered_filename+\"for_ml\"\n","            val_covered_for_cognates_filename = val_covered_filename+\"for_cognates\"\n","            with open(val_covered_filename, 'r', encoding=\"utf-8\") as f,                 open(val_covered_for_ml_filename, 'w', encoding=\"utf-8\") as f_ml,                 open(val_covered_for_cognates_filename, 'w', encoding=\"utf-8\") as f_cog:\n","                for line in f:\n","                    print(line.rstrip(), file=f_cog if cognates_tool.has_cognates(line) else f_ml)\n","\n","            # predict analysis for cognates non-having, copy analysis from cognates for cognates-having\n","            val_pred_for_ml_output_filename = val_pred_output_filename+\"for_ml\"\n","            val_pred_for_cognates_output_filename = val_pred_output_filename+\"for_cognates\"\n","            \n","            mlUtil.predict(model_filename, val_res_src_for_ml_filename, val_covered_for_ml_filename, val_pred_for_ml_output_filename)\n","            cognates_tool.predict(val_covered_for_cognates_filename, val_pred_for_cognates_output_filename)\n","            \n","            # merge prediction of both approaches\n","            get_ipython().system(f'cat {val_pred_for_ml_output_filename} >> {val_pred_output_filename} '                                  + '&& '                                  + f'cat {val_pred_for_cognates_output_filename} >> {val_pred_output_filename}')\n","            \n","            get_ipython().system(f'echo \"*===QUALITY ON VAL DATA===*\" >> {score_log_filename}')\n","            score_predictions(val_pred_output_filename, val_uncovered_filename, score_log_filename, dataEvaluator)\n","\n","            # log eval results\n","            get_ipython().system(f'cat {score_log_filename}')\n","            \n","            # send eval to @oserikov at telegram\n","            if telegram_notifications_enabled:\n","                telegram_message = f\"#score\\n{lang}\\n{track}\\n\"+''.join(open(score_log_filename).readlines())+'\\n'+EXP_DESCRIPTION\n","\n","                telegram_message_encoded = urllib.parse.quote(telegram_message)\n","                get_ipython().system(f'curl -i -X GET \"https://api.telegram.org/bot{bot_token}/sendMessage?chat_id={chat_id}&text={telegram_message_encoded}&parse_mode=markdown\"')\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["ml(langs, tracks, train_params, pred_params, TrainDataModifyer, NBestDataModifyer, DataEvaluator)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":["  # sandbox"],"metadata":{}},{"source":["\n","\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}